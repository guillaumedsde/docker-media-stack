%YAML 1.1
---
# THE MEDIA STACK #
# this docker-compose file will get a selection of media downloading/management webapps
# I've tried to simplify getting started as much as possible
#   1. configure your desired variables in the .env file
#   2. run `docker compose up -d` from the folder with this docker-copose.yml folder
# NOTE this process is not fully automated and individual configuration of webapps is still required after docker compose

# basic common configuration for most containers
x-conf: &conf
  TZ: ${TZ}
  PGID: ${GID}
  PUID: ${UID}
  # NOTE: this umask env var is mostly used by lsio containers
  #       docker does not have an option to set umask, see:
  #       https://github.com/moby/moby/issues/19189
  UMASK: "077"

# container restart policy
x-common: &common
  restart: always
  logging:
    driver: "local"
    options:
      max-file: "5"
      max-size: "10m"

# Traefik
x-traefik: &traefik
  traefik.enable: "true"

services:
  # reverse proxy
  traefik:
    image: docker.io/traefik:v3.3.5@sha256:104204dadedf5d1284f8ef8f97f705649ac81aa6f7a6c9abf13e2c59245b8abc
    user: ${UID}:${GID}
    depends_on:
      traefik-socket-proxy:
        condition: service_healthy
      authelia:
        condition: service_healthy
      prometheus:
        condition: service_healthy
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
    environment:
      TZ: ${TZ}
      TRAEFIK_GLOBAL_CHECKNEWVERSION: "false"
      TRAEFIK_GLOBAL_SENDANONYMOUSUSAGE: "false"
      # NOTE: internal
      TRAEFIK_PING_MANUALROUTING: "true"
      TRAEFIK_API: "true"
      TRAEFIK_API_DISABLEDASHBOARDAD: "true"
      # NOTE: providers
      TRAEFIK_PROVIDERS_DOCKER: "true"
      TRAEFIK_PROVIDERS_DOCKER_ENDPOINT: "tcp://traefik-socket-proxy:2375"
      TRAEFIK_PROVIDERS_DOCKER_EXPOSEDBYDEFAULT: "false"
      TRAEFIK_PROVIDERS_FILE_FILENAME: "/etc/traefik/dynamic_configuration.yml"
      # NOTE: entrypoints
      TRAEFIK_ENTRYPOINTS_WEB: "true"
      TRAEFIK_ENTRYPOINTS_WEB_ADDRESS: ":80"
      TRAEFIK_ENTRYPOINTS_WEB_HTTP_REDIRECTIONS_ENTRYPOINT_TO: "websecure"
      TRAEFIK_ENTRYPOINTS_WEBSECURE: "true"
      TRAEFIK_ENTRYPOINTS_WEBSECURE_ADDRESS: ":443"
      TRAEFIK_ENTRYPOINTS_WEBSECURE_HTTP3: "true"
      TRAEFIK_ENTRYPOINTS_WEBSECURE_HTTP_TLS_CERTRESOLVER: "le"
      TRAEFIK_ENTRYPOINTS_WEBSECURE_HTTP_TLS_DOMAINS_0_MAIN: "${root_domain}"
      TRAEFIK_ENTRYPOINTS_WEBSECURE_HTTP_TLS_DOMAINS_0_SANS: "*.${root_domain}"
      TRAEFIK_ENTRYPOINTS_WEBSECURE_HTTP_MIDDLEWARES: "securityHeaders@file"
      TRAEFIK_ENTRYPOINTS_WEBSECURE_TRANSPORT_RESPONDINGTIMEOUTS_READTIMEOUT: "600" # 10min
      # NOTE: certificate resolver
      TRAEFIK_CERTIFICATESRESOLVERS_LE: "true"
      TRAEFIK_CERTIFICATESRESOLVERS_LE_ACME_DNSCHALLENGE: "true"
      TRAEFIK_CERTIFICATESRESOLVERS_LE_ACME_DNSCHALLENGE_PROPAGATION: "30"
      TRAEFIK_CERTIFICATESRESOLVERS_LE_ACME_DNSCHALLENGE_PROVIDER: "ovh"
      TRAEFIK_CERTIFICATESRESOLVERS_LE_ACME_DNSCHALLENGE_RESOLVERS: "1.1.1.1,8.8.8.8"
      TRAEFIK_CERTIFICATESRESOLVERS_LE_ACME_KEYTYPE: "EC384"
      TRAEFIK_CERTIFICATESRESOLVERS_LE_ACME_EMAIL: ""
      TRAEFIK_CERTIFICATESRESOLVERS_LE_ACME_STORAGE: "/traefik/acme.json"
      OVH_ENDPOINT: ${OVH_ENDPOINT}
      OVH_APPLICATION_KEY: ${OVH_APPLICATION_KEY}
      OVH_APPLICATION_SECRET: ${OVH_APPLICATION_SECRET}
      OVH_CONSUMER_KEY: ${OVH_CONSUMER_KEY}
      LEGO_EXPERIMENTAL_CNAME_SUPPORT: "true"
      # NOTE: access log
      TRAEFIK_ACCESSLOG: "true"
      TRAEFIK_ACCESSLOG_ADDINTERNALS: "true"
      # NOTE: fields
      TRAEFIK_ACCESSLOG_FIELDS_DEFAULTMODE: "drop"
      TRAEFIK_ACCESSLOG_FIELDS_NAMES_CLIENTHOST: "keep"
      TRAEFIK_ACCESSLOG_FIELDS_NAMES_CLIENTUSERNAME: "keep"
      TRAEFIK_ACCESSLOG_FIELDS_NAMES_REQUESTHOST: "keep"
      TRAEFIK_ACCESSLOG_FIELDS_NAMES_REQUESTSCHEME: "keep"
      TRAEFIK_ACCESSLOG_FIELDS_NAMES_REQUESTLINE: "keep"
      TRAEFIK_ACCESSLOG_FIELDS_NAMES_REQUESTCONTENTSIZE: "keep"
      TRAEFIK_ACCESSLOG_FIELDS_NAMES_DOWNSTREAMSTATUS: "keep"
      TRAEFIK_ACCESSLOG_FIELDS_NAMES_DOWNSTREAMCONTENTSIZE: "keep"
      TRAEFIK_ACCESSLOG_FIELDS_NAMES_OVERHEAD: "keep"
      TRAEFIK_ACCESSLOG_FIELDS_NAMES_TLSVERSION: "keep"
      # NOTE: headers
      TRAEFIK_ACCESSLOG_FIELDS_HEADERS_DEFAULTMODE: "keep"
      TRAEFIK_ACCESSLOG_FIELDS_HEADERS_NAMES_AUTHORIZATION: "redact"
      # NOTE: metrics
      TRAEFIK_METRICS_OTLP: "true"
      TRAEFIK_METRICS_OTLP_HTTP: "true"
      TRAEFIK_METRICS_OTLP_HTTP_ENDPOINT: "http://prometheus:9090/api/v1/otlp/v1/metrics"
    labels:
      <<: *traefik
      # Dashboard
      traefik.http.routers.dashboard.rule: Host(`traefik.${root_domain}`)
      traefik.http.routers.dashboard.service: api@internal
      traefik.http.routers.dashboard.middlewares: authelia@docker
      # Healthcheck
      traefik.http.routers.ping.rule: Host(`ping.${root_domain}`)
      traefik.http.routers.ping.service: ping@internal
    read_only: true
    volumes:
      - ${docker_data_folder}/traefik/data:/traefik
      - ./config/traefik/dynamic_configuration.yml:/etc/traefik/dynamic_configuration.yml:ro
    tmpfs:
      - /tmp:mode=770,size=5M,uid=${UID},gid=${GID}
    networks:
      - traefik-socket-proxy
      - traefik
      - homer
      - nextcloud-proxy
      - robots
      - scrutiny
      - authelia-proxy
      - librespeed
      - tandoor
      - miniflux-proxy
      - lldap
      - prometheus
      - grafana
    <<: *common
    mem_limit: 500M
    memswap_limit: 500M
    cpu_count: 4
    ports:
      - target: 80
        published: 80
        host_ip: "0.0.0.0"
        protocol: tcp
        mode: host
      - target: 443
        published: 443
        host_ip: "0.0.0.0"
        protocol: tcp
        mode: host
      - target: 443
        published: 443
        host_ip: "0.0.0.0"
        protocol: udp
        mode: host
      - target: 80
        published: 80
        host_ip: "::"
        protocol: tcp
        mode: host
      - target: 443
        published: 443
        host_ip: "::"
        protocol: tcp
        mode: host
      - target: 443
        published: 443
        host_ip: "::"
        protocol: udp
        mode: host
    healthcheck:
      test:
        - "CMD"
        - "nc"
        - "-vz"
        - "127.0.0.1"
        - "80"
      start_period: 3s
      interval: 3s
      timeout: 3s
      retries: 5

  traefik-socket-proxy: &socket-proxy
    <<: *common
    image: ghcr.io/wollomatic/socket-proxy:1.6.1@sha256:b1298b4e8b600805e6873205c1cc317b250e4ab169858e9110dd5086fb4a24fd
    cpu_count: 2
    mem_limit: 50M
    memswap_limit: 50M
    user: ${UID}:${docker_group_id}
    read_only: true
    cap_drop:
      - ALL
    security_opt:
      - no-new-privileges
    volumes:
      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock
        read_only: true
    entrypoint:
      - "/socket-proxy"
      - "-loglevel=info"
      - "-listenip=0.0.0.0"
      - "-shutdowngracetime=5"
      - "-watchdoginterval=600"
      - "-stoponwatchdog"
      - "-allowhealthcheck"
    command:
      - "-allowfrom=traefik"
      # NOTE: https://github.com/wollomatic/traefik-hardened/blob/b2bfcad2a1bf727f2fea33d3d68d3eca9bb1f8da/docker-compose.yaml#L12
      - '-allowGET=/v1\..{1,2}/(version|containers/.*|events.*)'
    networks:
      - traefik-socket-proxy
    healthcheck:
      test:
        - "CMD"
        - "./healthcheck"
      interval: 10s
      timeout: 5s
      retries: 2

  # watchtower container to update images
  # NOTE to update the containers images' watchtower needs to restart the containers this implies some downtime when it does
  # NOTE watchtower also updates itself
  watchtower:
    image: ghcr.io/containrrr/watchtower:1.7.1@sha256:f9086bfda061100361fc2bacf069585678d760d705cf390918ccdbda8a00980b
    <<: *common
    mem_limit: 100M
    memswap_limit: 100M
    cpu_count: 2
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    secrets:
      - watchtower-metrics-password
    environment:
      DOCKER_HOST: "tcp://watchtower-socket-proxy:2375"
      WATCHTOWER_CLEANUP: "true"
      WATCHTOWER_SCHEDULE: "0 0 5 * * *"
      WATCHTOWER_HTTP_API_METRICS: "true"
      WATCHTOWER_HTTP_API_TOKEN: "/run/secrets/watchtower-metrics-password"
    depends_on:
      watchtower-socket-proxy:
        condition: service_healthy
    networks:
      - watchtower
      - watchtower-socket-proxy
      - prometheus-watchtower

  watchtower-socket-proxy:
    <<: *socket-proxy
    command:
      - "-allowfrom=watchtower"
      # NOTE: https://github.com/wollomatic/socket-proxy/blob/6ba2fa6112fb9a115e99de0634b15f1fbe1fb513/examples/docker-compose/watchtower/compose.yaml#L9-L12
      - '-allowGET=/v1\..{2}/(containers/.*|images/.*)'
      - '-allowPOST=/v1\..{2}/(containers/.*|images/.*|networks/.*)'
      - '-allowDELETE=/v1\..{2}/(containers/.*|images/.*)'
    networks:
      - watchtower-socket-proxy

  wireguard-tun-device-creator:
    image: docker.io/busybox:1-musl
    cpu_count: 2
    mem_limit: 20M
    memswap_limit: 20M
    read_only: true
    network_mode: "none"
    volumes:
      - type: bind
        source: ${docker_data_folder}/gluetun
        target: /gluetun
    entrypoint:
      - "/bin/sh"
      - "-c"
    command:
      - |
        rm -f "/gluetun/tun"
        mknod "/gluetun/tun" c 10 200

  # Wireguard VPN client
  wireguard:
    <<: *common
    depends_on:
      wireguard-tun-device-creator:
        condition: service_completed_successfully
    image: docker.io/qmcgaw/gluetun:v3.40.0@sha256:2b42bfa046757145a5155acece417b65b4443c8033fb88661a8e9dcf7fda5a00
    cap_drop:
      - ALL
    cap_add:
      - NET_ADMIN
    cpu_count: 4
    mem_limit: 250M
    memswap_limit: 250M
    sysctls:
      net.ipv6.conf.all.disable_ipv6: 0
      net.ipv4.conf.all.src_valid_mark: 1
    volumes:
      - type: bind
        source: ${docker_data_folder}/gluetun
        target: /gluetun
    devices:
      - "${docker_data_folder}/gluetun/tun:/dev/net/tun"
    networks:
      traefik:
        aliases:
          - qbittorrent
          - prowlarr
      flaresolverr:
    healthcheck:
      retries: 25
      start_period: 30s
    environment:
      <<: *conf
      LOG_LEVEL: ${GLUETUN_LOG_LEVEL:-info}
      VPN_SERVICE_PROVIDER: ${VPN_SERVICE_PROVIDER}
      VPN_TYPE: wireguard
      SERVER_COUNTRIES: ${VPN_SERVER_COUNTRIES}
      WIREGUARD_PRIVATE_KEY: ${WIREGUARD_PRIVATE_KEY}
      WIREGUARD_PRESHARED_KEY: ${WIREGUARD_PRESHARED_KEY}
      WIREGUARD_ADDRESSES: ${WIREGUARD_ADDRESSES}
      FIREWALL_VPN_INPUT_PORTS: ${FIREWALL_VPN_INPUT_PORTS}
      # Allow LAN access to other containers
      FIREWALL_OUTBOUND_SUBNETS: ${FIREWALL_OUTBOUND_SUBNETS}
      DNS_KEEP_NAMESERVER: "on"
      VERSION_INFORMATION: "off"
      HEALTH_SUCCESS_WAIT_DURATION: "10m"
    labels:
      <<: *traefik
      traefik.docker.network: "${COMPOSE_PROJECT_NAME}_traefik"
      # Prowlarr
      # service
      traefik.http.services.prowlarr.loadbalancer.server.port: "9696"
      # Router
      traefik.http.routers.prowlarr.rule: Host(`prowlarr.${root_domain}`)
      traefik.http.routers.prowlarr.service: prowlarr
      # Forward auth
      traefik.http.routers.prowlarr.middlewares: authelia@docker
      # qBittorrent
      # service
      traefik.http.services.qbittorrent.loadbalancer.server.port: "8080"
      # router
      traefik.http.routers.qbittorrent.rule: Host(`torrent.${root_domain}`)
      traefik.http.routers.qbittorrent.service: qbittorrent
      # Forward auth
      traefik.http.routers.qbittorrent.middlewares: authelia@docker
      # Disable watchtower for this container because of these issues
      # https://github.com/containrrr/watchtower/issues/188
      # https://github.com/containrrr/watchtower/issues/1013
      com.centurylinklabs.watchtower.enable: "false"

  # Torrent client
  qbittorrent:
    <<: *common
    depends_on:
      wireguard:
        restart: true
        condition: service_healthy
    volumes:
      - ${docker_data_folder}/qbittorrent:/config
      - ${data_folder}:/data
      - ${docker_data_folder}/blackhole:/blackhole
      - /etc/localtime:/etc/localtime:ro
    tmpfs:
      - /tmp:mode=770,size=5M,uid=${UID},gid=${GID}
    read_only: true
    cpu_count: 6
    mem_limit: 10G
    memswap_limit: 10G
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    # NOTE: use `services:` instead of `containers:` because the
    #       container is defined in the same compose file
    network_mode: service:wireguard
    image: ghcr.io/guillaumedsde/qbittorrent-distroless@sha256:5a3f42af70b379e598fe1aabce2122b75365b735e6e0fd2bf7ce2deba1525069
    labels:
      com.centurylinklabs.watchtower.depends-on: ${COMPOSE_PROJECT_NAME}-wireguard-1
    healthcheck:
      start_period: 30s
      retries: 10

  prowlarr:
    <<: *common
    image: ghcr.io/elfhosted/prowlarr-develop:1.33.3.5008@sha256:3b027ea4148db2e86dc7f489141a7e87cdacedbfb67d5c0bea8f9f84d7d582be
    cpu_count: 4
    mem_limit: 3G
    memswap_limit: 3G
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    network_mode: service:wireguard
    depends_on:
      wireguard:
        restart: true
        condition: service_healthy
    volumes:
      - ${docker_data_folder}/prowlarr:/config
      - ${docker_data_folder}/blackhole:/blackhole
    environment:
      <<: *conf
    labels:
      com.centurylinklabs.watchtower.depends-on: ${COMPOSE_PROJECT_NAME}-wireguard-1
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:9696/ping
      interval: 10s
      timeout: 5s
      retries: 8
      start_period: 15s

  flaresolverr:
    image: ghcr.io/flaresolverr/flaresolverr:v3.3.21@sha256:1a30e1ad6bb3df626bfe3b9735a6d60e208c475c89bf18be0db4d4b121a3cb0e
    <<: *common
    # NOTE: default image UID is 1000
    user: 1000:1000
    cpu_count: 4
    mem_limit: 4G
    memswap_limit: 4G
    cap_drop:
      - ALL
    environment:
      <<: *conf
      PROMETHEUS_ENABLED: "true"
    networks:
      - flaresolverr
      - prometheus-flaresolverr

  # radarr, webapp for downloading movies through torrent client
  radarr:
    image: ghcr.io/elfhosted/radarr:5.21.1.9799@sha256:c5a4213efe7cff3753ad330f9d5fcfd4d1f07bdc22f61acd87eb138d9df1bee6
    <<: *common
    cpu_count: 4
    mem_limit: 2G
    memswap_limit: 2G
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    depends_on:
      qbittorrent:
        condition: service_healthy
      prowlarr:
        condition: service_healthy
    volumes:
      - ${docker_data_folder}/radarr:/config
      - ${data_folder}:/data
    networks:
      - traefik
    environment:
      <<: *conf
    labels:
      <<: *traefik
      traefik.http.routers.radarr.rule: Host(`movies.${root_domain}`)
      # service
      traefik.http.services.radarr.loadbalancer.server.port: "7878"
      # Forward auth
      traefik.http.routers.radarr.middlewares: authelia@docker
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:7878/ping
      interval: 5s
      timeout: 4s
      retries: 4
      start_period: 5s

  # sonarr, webapp for downloading series through torrent client
  sonarr:
    <<: *common
    image: ghcr.io/elfhosted/sonarr:4.0.14.2939@sha256:7b8473c842e4196125a8d37f2f384dd1538ac2fbe02e50d2a1be24461f057c8c
    cpu_count: 4
    mem_limit: 2G
    memswap_limit: 2G
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    depends_on:
      qbittorrent:
        condition: service_healthy
      prowlarr:
        condition: service_healthy
    volumes:
      - ${docker_data_folder}/sonarr:/config
      - ${data_folder}:/data
    networks:
      - traefik
    environment:
      <<: *conf
      # NOTE: fix for read-only filesystems, see:
      #       https://stackoverflow.com/questions/74447989/failed-to-create-coreclr-hresult-0x80004005#comment131446515_74447989
      COMPlus_EnableDiagnostics: "0"
    labels:
      <<: *traefik
      traefik.http.routers.sonarr.rule: Host(`series.${root_domain}`)
      # service
      traefik.http.services.sonarr.loadbalancer.server.port: "8989"
      # Forward auth
      traefik.http.routers.sonarr.middlewares: authelia@docker
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:8989/ping
      interval: 5s
      timeout: 4s
      retries: 4
      start_period: 5s

  # lidarr, webapp for downloading music through torrent client
  lidarr:
    <<: *common
    image: ghcr.io/elfhosted/lidarr:2.10.3.4602@sha256:fbed293b810c6d957bb185f9584d2a7bdef44fba42cc06be2736f7b9c1f3f1cd
    cpu_count: 4
    mem_limit: 2G
    memswap_limit: 2G
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    depends_on:
      qbittorrent:
        condition: service_healthy
      prowlarr:
        condition: service_healthy
    volumes:
      - ${docker_data_folder}/lidarr:/config
      - ${data_folder}:/data
    networks:
      - traefik
    environment:
      <<: *conf
      # NOTE: fix for read-only filesystems, see:
      #       https://stackoverflow.com/questions/74447989/failed-to-create-coreclr-hresult-0x80004005#comment131446515_74447989
      COMPlus_EnableDiagnostics: "0"
    labels:
      <<: *traefik
      traefik.http.routers.lidarr.rule: Host(`music.${root_domain}`)
      # service
      traefik.http.services.lidarr.loadbalancer.server.port: "8686"
      # Forward auth
      traefik.http.routers.lidarr.middlewares: authelia@docker
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:8686/ping
      interval: 5s
      timeout: 4s
      retries: 4
      start_period: 5s

  # readarr, webapp for downloading books through torrent client
  readarr:
    <<: *common
    image: ghcr.io/elfhosted/readarr-develop:0.4.13.2760@sha256:88a56f6f79e301dcaf802c3e2790e604a5b81dd3b8b00efef1b89c6ff6d81c94
    cpu_count: 4
    mem_limit: 2G
    memswap_limit: 2G
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    depends_on:
      qbittorrent:
        condition: service_healthy
      prowlarr:
        condition: service_healthy
    volumes:
      - ${docker_data_folder}/readarr:/config
      - ${data_folder}:/data
    networks:
      - traefik
    environment:
      <<: *conf
      # NOTE: fix for read-only filesystems, see:
      #       https://stackoverflow.com/questions/74447989/failed-to-create-coreclr-hresult-0x80004005#comment131446515_74447989
      COMPlus_EnableDiagnostics: "0"
    labels:
      <<: *traefik
      traefik.http.routers.readarr.rule: Host(`books.${root_domain}`)
      # service
      traefik.http.services.readarr.loadbalancer.server.port: "8787"
      # Forward auth
      traefik.http.routers.readarr.middlewares: authelia@docker
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:8787/ping
      interval: 5s
      timeout: 4s
      retries: 4
      start_period: 5s

  # bazarr, webapp for downloading subtitles
  bazarr:
    <<: *common
    depends_on:
      radarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    image: ghcr.io/elfhosted/bazarr:1.5.1@sha256:4ce55b7dcc6a07d379a5cc30163d9426797e2c5a4d8a35e99d6c6aa624788451
    cpu_count: 2
    mem_limit: 500M
    memswap_limit: 500M
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    environment:
      <<: *conf
    volumes:
      - ${docker_data_folder}/bazarr:/config
      - ${data_folder}:/data
    tmpfs:
      - /tmp:mode=770,size=10M,uid=${UID},gid=${GID}
    networks:
      - traefik
    labels:
      <<: *traefik
      traefik.http.routers.bazarr.rule: Host(`subtitles.${root_domain}`)
      # service
      traefik.http.services.bazarr.loadbalancer.server.port: "6767"
      # Forward auth
      traefik.http.routers.bazarr.middlewares: authelia@docker
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://127.0.0.1:6767
      interval: 5s
      timeout: 4s
      retries: 4
      start_period: 5s

  # automatically extract torrented archives
  unpackerr:
    <<: *common
    image: docker.io/golift/unpackerr:0.14.5@sha256:8493ffc2dd17e0b8a034552bb52d44e003fa457ee407da97ccc69328bce4a815
    user: ${UID}:${GID}
    read_only: true
    cpu_count: 4
    mem_limit: 500M
    memswap_limit: 500M
    cap_drop:
      - ALL
    networks:
      - traefik
    volumes:
      - ${data_folder}:/data
    environment:
      UN_SONARR_0_URL: http://sonarr:8989
      UN_SONARR_0_API_KEY: ${SONARR_API_KEY}
      UN_SONARR_0_PATH: /data/download/series
      UN_RADARR_0_URL: http://radarr:7878
      UN_RADARR_0_API_KEY: ${RADARR_API_KEY}
      UN_RADARR_0_PATH: /data/download/series
      UN_LIDARR_0_URL: http://lidarr:8686
      UN_LIDARR_0_API_KEY: ${LIDARR_API_KEY}
      UN_LIDARR_0_PATH: /data/download/music

  # homer is a homepage container
  homer:
    <<: *common
    image: docker.io/b4bz/homer:v25.04.1@sha256:602287607aa9ba98e317a9d5d31ce45fd37e016685d230bf7fb0dc9f4004e607
    user: ${UID}:${GID}
    cpu_count: 2
    mem_limit: 100M
    memswap_limit: 100M
    read_only: true
    cap_drop:
      - ALL
    networks:
      - homer
    volumes:
      - ${docker_data_folder}/homer/config.yml:/www/assets/config.yml:ro
      - ${docker_data_folder}/homer/assets:/www/assets:ro
    labels:
      <<: *traefik
      traefik.http.routers.homer.rule: Host(`home.${root_domain}`) || Host(`${root_domain}`)
      # service
      traefik.http.services.homer.loadbalancer.server.port: "8080"
      # Forward auth
      traefik.http.routers.homer.middlewares: authelia@docker

  # jellyfin media server
  jellyfin:
    <<: *common
    image: docker.io/jellyfin/jellyfin:10.10.7@sha256:7ae36aab93ef9b6aaff02b37f8bb23df84bb2d7a3f6054ec8fc466072a648ce2
    user: ${UID}:${GID}
    cpu_count: 15
    mem_limit: 8G
    memswap_limit: 8G
    read_only: true
    cap_drop:
      - ALL
    depends_on:
      lldap:
        condition: service_healthy
    tmpfs:
      - /config/data/transcodes:mode=770,size=4G,uid=${UID},gid=${GID}
      - /tmp:mode=770,size=10M,uid=${UID},gid=${GID}
    volumes:
      - type: bind
        source: ${docker_data_folder}/jellyfin
        target: /config
      - type: bind
        source: ${data_folder}/movies
        target: /data/movies
        read_only: true
      - type: bind
        source: ${data_folder}/series
        target: /data/series
        read_only: true
      - type: bind
        source: ${data_folder}/music
        target: /data/music
        read_only: true
      - type: bind
        source: "${data_folder}/Recettes de cuisine"
        target: /data/recipes
        read_only: true
    networks:
      - traefik
      - lldap
      - prometheus-jellyfin
    environment:
      <<: *conf
      # NOTE: required for read only root filesystem, see:
      #       https://github.com/dotnet/docs/issues/10217#issuecomment-462323277
      COMPlus_EnableDiagnostics: "0"
      #Uncomment forofficial Jellyfin image:
      JELLYFIN_DATA_DIR: /config/data
      JELLYFIN_CONFIG_DIR: /config
      JELLYFIN_LOG_DIR: /config/log
      JELLYFIN_CACHE_DIR: /config/cache
      JELLYFIN_FFmpeg__probesize: "2G"
      JELLYFIN_FFmpeg__analyzeduration: "800M"
      JELLYFIN_PublishedServerUrl: https://jellyfin.${root_domain}
    labels:
      <<: *traefik
      traefik.docker.network: "${COMPOSE_PROJECT_NAME}_traefik"
      traefik.http.routers.jellyfin.rule: Host(`jellyfin.${root_domain}`)
      # service
      traefik.http.services.jellyfin.loadbalancer.server.port: "8096"
      traefik.http.middlewares.jellyfin-deny-metrics.redirectregex.regex: "^https://jellyfin.${root_domain}/metrics$"
      traefik.http.middlewares.jellyfin-deny-metrics.redirectregex.replacement: "https://jellyfin.${root_domain}/"

  jellyseer:
    <<: *common
    image: docker.io/fallenbagel/jellyseerr:2.5.2@sha256:2a611369ad1d0d501c2d051fc89b6246ff081fb4a30879fdc75642cf6a37b1a6
    user: ${UID}:${GID}
    cpu_count: 2
    mem_limit: 750M
    memswap_limit: 750M
    read_only: true
    cap_drop:
      - ALL
    depends_on:
      jellyfin:
        condition: service_healthy
      radarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    volumes:
      - ${docker_data_folder}/jellyseer:/app/config
    tmpfs:
      - /tmp:mode=770,size=50M,uid=${UID},gid=${GID}
    networks:
      - traefik
    environment:
      <<: *conf
    labels:
      <<: *traefik
      traefik.http.middlewares.ombi-jellyseer-redirect.redirectregex.regex: "^https?://ombi.${root_domain}/(.*)"
      traefik.http.middlewares.ombi-jellyseer-redirect.redirectregex.replacement: "https://jellyseer.${root_domain}/"
      traefik.http.middlewares.ombi-jellyseer-redirect.redirectregex.permanent: "false"
      traefik.http.routers.jellyseer.rule: Host(`jellyseer.${root_domain}`) || Host(`ombi.${root_domain}`)
      # service
      traefik.http.services.jellyseer.loadbalancer.server.port: "5055"
      # Forward auth
      traefik.http.routers.jellyseer.middlewares: ombi-jellyseer-redirect
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:5055
      interval: 5s
      timeout: 4s
      retries: 4
      start_period: 5s

  # personal cloud
  nextcloud: &nextcloud
    <<: *common
    depends_on:
      nextcloud-postgres:
        condition: service_healthy
      nextcloud-redis:
        condition: service_healthy
      nextcloud-imaginary:
        condition: service_healthy
      lldap:
        condition: service_healthy
    mem_limit: 8G
    memswap_limit: 8G
    cpu_count: 8
    image: docker.io/nextcloud:31.0-apache
    user: ${NEXTCLOUD_UID}:${GID}
    cap_drop:
      - ALL
    read_only: true
    tmpfs:
      - /run:mode=770,size=5M,uid=${NEXTCLOUD_UID},gid=${GID}
      # NOTE: /tmp needs to be proportional to max uploadable file size, see:
      #       https://docs.nextcloud.com/server/latest/admin_manual/configuration_files/big_file_upload_configuration.html#system-configuration
      - /tmp:mode=770,size=4000M,uid=${NEXTCLOUD_UID},gid=${GID}
    volumes:
      - type: "bind"
        source: "${docker_data_folder}/nextcloud"
        target: "/var/www/html"
      - type: "bind"
        source: "${user_data_folder}"
        target: "/data/users"
      - type: "bind"
        source: "./config/nextcloud/redis_config_generator.sh"
        target: "/docker-entrypoint-hooks.d/before-starting/redis_config_generator.sh"
        read_only: true
    networks:
      - traefik
      - nextcloud
      - nextcloud-proxy
      - lldap
    environment:
      <<: *conf
      POSTGRES_DB: nextcloud
      POSTGRES_USER: nextcloud
      POSTGRES_PASSWORD: ${NEXTCLOUD_DB_PASSWORD}
      POSTGRES_HOST: nextcloud-postgres
      # NOTE: HOOK_ prefix is for env var to be used with /docker-entrypoint-hooks.d/before-starting scripts
      HOOK_REDIS_HOST: nextcloud-redis
      HOOK_REDIS_HOST_PASSWORD: ${NEXTCLOUD_REDIS_PASS}
      # NOTE: see https://www.php.net/manual/en/configuration.file.php
      PHP_INI_SCAN_DIR: "/tmp/php_conf_d:"
      SMTP_HOST: smtp.sendgrid.net
      SMTP_SECURE: ssl
      SMTP_PORT: "465"
      SMTP_AUTHTYPE: login
      SMTP_NAME: apikey
      SMTP_PASSWORD: ${SENDGRID_PASS}
      MAIL_FROM_ADDRESS: nextcloud
      MAIL_DOMAIN: ${root_domain}
      PHP_UPLOAD_LIMIT: 1G
      APACHE_BODY_LIMIT: "1073741824"
    labels:
      <<: *traefik
      traefik.docker.network: "${COMPOSE_PROJECT_NAME}_nextcloud-proxy"
      traefik.http.routers.nextcloud.rule: Host(`cloud.${root_domain}`)
      traefik.http.routers.nextcloud.middlewares: nextcloud-redirectregex
      # service
      traefik.http.services.nextcloud.loadbalancer.server.port: "80"
      traefik.http.middlewares.nextcloud-redirectregex.redirectregex.permanent: "true"
      traefik.http.middlewares.nextcloud-redirectregex.redirectregex.regex: https://cloud.${root_domain}/.well-known/(card|cal)dav
      traefik.http.middlewares.nextcloud-redirectregex.redirectregex.replacement: https://cloud.${root_domain}/remote.php/dav/
    healthcheck:
      test: curl -sSf 'http://localhost/status.php' | grep '"installed":true' | grep '"maintenance":false' | grep '"needsDbUpgrade":false' || exit 1
      interval: 10s
      timeout: 5s
      retries: 10

  nextcloud-cron:
    <<: *nextcloud
    restart: unless-stopped
    labels: {}
    entrypoint:
      - /bin/bash
      - -c
      - |
        set -euxo pipefail
        php /var/www/html/cron.php --verbose
        sleep 5m
    healthcheck: {}

  # database for nextcloud
  nextcloud-postgres:
    <<: *common
    image: docker.io/postgres:15-bookworm
    cpu_count: 2
    mem_limit: 2G
    memswap_limit: 2G
    shm_size: 128mb
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    environment:
      POSTGRES_PASSWORD: ${NEXTCLOUD_DB_PASSWORD}
      POSTGRES_USER: nextcloud
    tmpfs:
      - /run:mode=770,size=5M,uid=${UID},gid=${GID}
      - /tmp:mode=770,size=5M,uid=${UID},gid=${GID}
    volumes:
      - ${docker_data_folder}/nextcloud-postgres:/var/lib/postgresql/data
    networks:
      - nextcloud
    healthcheck: &postgres-healthcheck
      test:
        - "CMD-SHELL"
        - >-
          pg_isready
          --username="$${POSTGRES_USER}"
          --dbname="$${POSTGRES_USER}"
          --host=localhost
          --port=5432
      start_period: 5s
      interval: 5s
      timeout: 4s
      retries: 5

  # cache for nextcloud
  nextcloud-redis:
    <<: *common
    cpu_count: 2
    mem_limit: 250M
    memswap_limit: 250M
    read_only: true
    user: ${UID}:${GID}
    image: docker.io/valkey/valkey:8-alpine
    cap_drop:
      - ALL
    networks:
      - nextcloud
    volumes:
      - ${docker_data_folder}/nextcloud-redis:/data
    environment:
      REDISCLI_AUTH: "${NEXTCLOUD_REDIS_PASS}"
      VALKEY_EXTRA_FLAGS: "--requirepass ${NEXTCLOUD_REDIS_PASS}"
    healthcheck: &valkey-healthcheck
      test:
        - "CMD"
        - "valkey-cli"
        - "ping"

  nextcloud-imaginary:
    <<: *common
    image: docker.io/nextcloud/aio-imaginary:20250306_093458@sha256:be5df22530a03b5cd2dd8ec507f4651c076b407fee21cca7f578e459b16d0d7a
    cpu_count: 12
    mem_limit: 2500M
    memswap_limit: 2500M
    read_only: true
    user: ${UID}:${GID}
    cap_add:
      - SYS_NICE
    cap_drop:
      - ALL
    tmpfs:
      - /tmp:mode=770,size=2000M,uid=${UID},gid=${GID}
    command:
      - "-cpus=12"
      - "-enable-url-source"
      - "-concurrency=50"
    networks:
      - nextcloud
    healthcheck:
      start_period: 0s
      test: /healthcheck.sh
      interval: 30s
      timeout: 30s
      start_interval: 5s
      retries: 3

  miniflux:
    <<: *common
    depends_on:
      miniflux-db:
        condition: service_healthy
    image: ghcr.io/miniflux/miniflux:2.2.7-distroless@sha256:f2338de4f536b39f75e806cd5cf3da5b8149a5d4c4a4b03bf1824b932711eb56
    cpu_count: 2
    mem_limit: 500M
    memswap_limit: 500M
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    networks:
      - miniflux
      - miniflux-proxy
      - prometheus-miniflux
    secrets:
      - miniflux-metrics-password
    environment:
      BASE_URL: "https://miniflux.${root_domain}/"
      AUTH_PROXY_HEADER: "Remote-User"
      AUTH_PROXY_USER_CREATION: "true"
      RUN_MIGRATIONS: "1"
      DATABASE_URL: "postgres://miniflux:${MINIFLUX_DB_PASSWORD}@miniflux-db:5432/miniflux?sslmode=disable"
      METRICS_COLLECTOR: "1"
      # NOTE: be careful with this if removing forward auth
      METRICS_ALLOWED_NETWORKS: "0.0.0.0"
      METRICS_USERNAME: "miniflux-metrics"
      METRICS_PASSWORD_FILE: "/run/secrets/miniflux-metrics-password"
    labels:
      <<: *traefik
      traefik.docker.network: "${COMPOSE_PROJECT_NAME}_miniflux-proxy"
      traefik.http.routers.miniflux.rule: Host(`miniflux.${root_domain}`)
      # service
      traefik.http.services.miniflux.loadbalancer.server.port: "8080"
      # Forward auth
      traefik.http.routers.miniflux.middlewares: authelia@docker

  miniflux-db:
    <<: *common
    image: docker.io/postgres:16-bookworm
    cpu_count: 2
    mem_limit: 2G
    memswap_limit: 2G
    shm_size: 128mb
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    environment:
      POSTGRES_PASSWORD: ${MINIFLUX_DB_PASSWORD}
      POSTGRES_USER: miniflux
    tmpfs:
      - /run:mode=770,size=5M,uid=${UID},gid=${GID}
      - /tmp:mode=770,size=5M,uid=${UID},gid=${GID}
    volumes:
      - ${docker_data_folder}/miniflux-db:/var/lib/postgresql/data
    networks:
      - miniflux
    healthcheck: *postgres-healthcheck

  librespeed:
    <<: *common
    image: ghcr.io/librespeed/speedtest-rust:v1.3.7@sha256:c4d18c0fe1892efcbfc7cb917cfa0a5ffd5ba8b616243d980c5755cb4f08edce
    cpu_count: 2
    mem_limit: 500M
    memswap_limit: 500M
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    volumes:
      - type: bind
        source: "./config/librespeed-rs/config.toml"
        target: "/usr/local/bin/configs.toml"
        read_only: true
    networks:
      - librespeed
    labels:
      <<: *traefik
      traefik.http.routers.librespeed.rule: Host(`librespeed.${root_domain}`)
      # service
      traefik.http.services.librespeed.loadbalancer.server.port: "8080"
      # Forward auth
      traefik.http.routers.librespeed.middlewares: authelia@docker

  tandoor-db:
    <<: *common
    image: docker.io/postgres:16-bookworm
    cpu_count: 2
    mem_limit: 500M
    memswap_limit: 500M
    shm_size: 128mb
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    environment:
      POSTGRES_USER: tandoor
      POSTGRES_PASSWORD: ${TANDOOR_DB_PASSWORD}
    tmpfs:
      - /run:mode=770,size=5M,uid=${UID},gid=${GID}
      - /tmp:mode=770,size=5M,uid=${UID},gid=${GID}
    volumes:
      - ${docker_data_folder}/tandoor-db:/var/lib/postgresql/data
    networks:
      - tandoor
    healthcheck: *postgres-healthcheck

  tandoor:
    <<: *common
    image: docker.io/vabene1111/recipes:1.5.34@sha256:7d083f3af3a2a58bb8bb5fe94d59a37e2076bc405cf9bd51c2e3eaa18d6baf5a
    cpu_count: 2
    mem_limit: 750M
    memswap_limit: 750M
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    depends_on:
      tandoor-db:
        condition: service_healthy
    environment:
      SECRET_KEY: ${TANDOOR_SECRET_KEY}
      DB_ENGINE: django.db.backends.postgresql
      POSTGRES_HOST: tandoor-db
      POSTGRES_PORT: "5432"
      POSTGRES_USER: tandoor
      POSTGRES_PASSWORD: ${TANDOOR_DB_PASSWORD}
      POSTGRES_DB: tandoor
      REMOTE_USER_AUTH: "1"
    volumes:
      - type: bind
        source: "${docker_data_folder}/tandoor/staticfiles"
        target: "/opt/recipes/staticfiles"
      - type: bind
        source: "${docker_data_folder}/tandoor/mediafiles"
        target: "/opt/recipes/mediafiles"
    tmpfs:
      - /tmp:mode=770,size=10M,uid=${UID},gid=${GID}
    networks:
      - tandoor
    labels:
      <<: *traefik
      traefik.http.routers.tandoor.rule: Host(`tandoor.${root_domain}`)
      # service
      traefik.http.services.tandoor.loadbalancer.server.port: "8080"
      # Forward auth
      traefik.http.routers.tandoor.middlewares: authelia@docker
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:8080
      interval: 5s
      timeout: 4s
      retries: 4
      start_period: 5s

  robots:
    <<: *common
    image: docker.io/nginxinc/nginx-unprivileged:1.27-alpine-slim
    mem_limit: 200M
    memswap_limit: 200M
    cpu_count: 2
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    tmpfs:
      - /var/cache/nginx:mode=770,size=5M,uid=${UID},gid=${GID}
      - /var/run:mode=770,size=5M,uid=${UID},gid=${GID}
      - /tmp:mode=770,size=5M,uid=${UID},gid=${GID}
    volumes:
      - type: bind
        source: ./config/nginx/default.conf
        target: /etc/nginx/conf.d/default.conf
        read_only: true
      - type: bind
        source: ./config/nginx/robots.txt
        target: /usr/share/nginx/html/robots.txt
        read_only: true
    networks:
      - robots
    labels:
      <<: *traefik
      traefik.http.routers.robots.rule: ( Host(`${root_domain}`) || HostRegexp(`{subhost:[a-z]+}.${root_domain}`) ) && Path(`/robots.txt`)
      # service
      traefik.http.services.robots.loadbalancer.server.port: "8080"
    healthcheck:
      test:
        - "CMD"
        - "nc"
        - "-vz"
        - "127.0.0.1"
        - "8080"
      start_period: 3s
      interval: 1s
      timeout: 1s
      retries: 5

  ##################################################################
  # Monitoring

  scrutiny:
    image: ghcr.io/analogj/scrutiny:v0.8.1-omnibus@sha256:214261df881879941b4b3e2235c601b086b358d0a29b513a82c4bf1e2de68e3e
    <<: *common
    mem_limit: 500M
    memswap_limit: 500M
    cpu_count: 2
    cap_add:
      - SYS_RAWIO
    devices:
      - /dev/sda
      - /dev/sdb
      - /dev/sdc
      - /dev/sdd
      - /dev/sde
      - /dev/sdf
      - /dev/sdg
      - /dev/sdh
      - /dev/sdi
      - /dev/sdj
      - /dev/sdk
      - /dev/sdl
    volumes:
      - /run/udev:/run/udev:ro
      - ${docker_data_folder}/scrutiny/scrutiny:/opt/scrutiny/config
      - ${docker_data_folder}/scrutiny/influxdb:/opt/scrutiny/influxdb
    networks:
      - scrutiny
    environment:
      <<: *conf
    labels:
      <<: *traefik
      traefik.http.routers.scrutiny.rule: Host(`scrutiny.${root_domain}`)
      # service
      traefik.http.services.scrutiny.loadbalancer.server.port: "8080"
      # Forward auth
      traefik.http.routers.scrutiny.middlewares: authelia@docker
    healthcheck:
      test:
        - CMD
        - curl
        - -sSf
        - http://localhost:8080
      interval: 5s
      timeout: 4s
      retries: 4
      start_period: 5s

  prometheus:
    <<: *common
    image: docker.io/prom/prometheus:v3.2.1@sha256:6927e0919a144aa7616fd0137d4816816d42f6b816de3af269ab065250859a62
    mem_limit: 500M
    memswap_limit: 500M
    cpu_count: 4
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    networks:
      - prometheus
      - prometheus-flaresolverr
      - prometheus-jellyfin
      - prometheus-authelia
      - prometheus-miniflux
      - prometheus-watchtower
    extra_hosts:
      # NOTE: make host accessible to scrape node-exporter metrics
      host.docker.internal: host-gateway
    secrets:
      - miniflux-metrics-password
      - watchtower-metrics-password
    volumes:
      - type: bind
        source: "./config/prometheus/prometheus.yml"
        target: "/etc/prometheus/prometheus.yml"
        read_only: true
      - type: bind
        source: "${docker_data_folder}/prometheus"
        target: "/prometheus"
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.enable-otlp-receiver"
    labels:
      <<: *traefik
      traefik.http.routers.prometheus.rule: Host(`prometheus.${root_domain}`)
      traefik.http.routers.prometheus.middlewares: authelia@docker
      # service
      traefik.http.services.prometheus.loadbalancer.server.port: "9090"
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:9090/-/ready
      interval: 10s
      timeout: 5s
      retries: 8
      start_period: 10s

  grafana:
    <<: *common
    depends_on:
      prometheus:
        condition: service_healthy
    image: docker.io/grafana/grafana:11.6.0@sha256:62d2b9d20a19714ebfe48d1bb405086081bc602aa053e28cf6d73c7537640dfb
    mem_limit: 500M
    memswap_limit: 500M
    cpu_count: 4
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    networks:
      - grafana
      - prometheus
    volumes:
      - type: bind
        source: "${docker_data_folder}/grafana"
        target: "/var/lib/grafana"
      - type: bind
        source: "./config/grafana/dashboard_definitions"
        target: "/var/lib/grafana/dashboards"
        read_only: true
      - type: bind
        source: "./config/grafana/provisioning"
        target: "/etc/grafana/provisioning"
        read_only: true
    environment:
      HOSTNAME: "grafana.${root_domain}"
      GF_SERVER_ROOT_URL: "https://grafana.${root_domain}"
      GF_AUTH_PROXY_ENABLED: "true"
      GF_AUTH_PROXY_HEADER_NAME: "Remote-User"
      GF_AUTH_PROXY_HEADER_ENABLE_LOGIN_TOKEN: "false"
      GF_AUTH_DISABLE_SIGNOUT_MENU: "true"
      GF_USERS_AUTO_ASSIGN_ORG: "true"
      GF_USERS_AUTO_ASSIGN_ORG_ROLE: "Viewer"
      GF_SECURITY_DISABLE_INITIAL_ADMIN_CREATION: "true"
      GF_SECURITY_DISABLE_GRAVATAR: "true"
      GF_SECURITY_ANGULAR_SUPPORT_ENABLED: "false"
      GF_DATE_FORMATS_USE_BROWSER_LOCALE: "true"
      GF_DATE_FORMATS_DEFAULT_TIMEZONE: "browser"
      GF_NEWS_NEWS_FEED_ENABLED: "false"
      GF_PROFILE_ENABLED: "false"
      GF_HELP_ENABLED: "false"
      GF_METRICS_ENABLED: "false"
      GF_ANALYTICS_ENABLED: "false"
      GF_ANALYTICS_REPORTING_ENABLED: "false"
      GF_ANALYTICS_CHECK_FOR_UPDATES: "false"
      GF_ANALYTICS_CHECK_FOR_PLUGIN_UPDATES: "false"
      GF_ANALYTICS_FEEDBACK_LINKS_ENABLED: "false"
      GF_SNAPSHOTS_ENABLED: "false"
      GF_DISABLE_SIGNOUT_MENU: "true"
    labels:
      <<: *traefik
      traefik.docker.network: "${COMPOSE_PROJECT_NAME}_grafana"
      traefik.http.routers.grafana.rule: Host(`grafana.${root_domain}`)
      traefik.http.routers.grafana.middlewares: authelia@docker
      # service
      traefik.http.services.grafana.loadbalancer.server.port: "3000"
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:3000/api/health
      interval: 10s
      timeout: 5s
      retries: 8
      start_period: 10s

  node-exporter:
    <<: *common
    image: docker.io/prom/node-exporter:v1.9.1@sha256:d00a542e409ee618a4edc67da14dd48c5da66726bbd5537ab2af9c1dfc442c8a
    mem_limit: 250M
    memswap_limit: 250M
    cpu_count: 2
    user: ${UID}:${GID}
    read_only: true
    pid: host
    cap_drop:
      - ALL
    volumes:
      - type: bind
        source: "/"
        target: "/host"
        read_only: true
        bind:
          propagation: "rslave"
    network_mode: host
    command:
      - "--path.rootfs=/host"
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:9100/
      interval: 10s
      timeout: 5s
      retries: 8
      start_period: 10s

  ##################################################################
  # Authentication

  lldap:
    <<: *common
    depends_on:
      lldap-db:
        condition: service_healthy
    image: "docker.io/lldap/lldap:v0.6.1-alpine-rootless@sha256:7e1c16090167c49d34e9e98f9f7d3fce97d3d86de8d2f62eedefad3dca3d270b"
    cpu_count: 2
    mem_limit: "250M"
    memswap_limit: "250M"
    user: "${UID}:${GID}"
    read_only: true
    cap_drop:
      - ALL
    volumes:
      - type: "bind"
        source: "${docker_data_folder}/lldap"
        target: /data
    networks:
      - "lldap"
      - "lldap-db"
    environment:
      <<: *conf
      LLDAP_LDAP_HOST: "::"
      LLDAP_LDAP_PORT: "3890"
      LLDAP_JWT_SECRET: "${LLDAP_JWT_SECRET}"
      LLDAP_KEY_SEED: "${LLDAP_KEY_SEED}"
      LLDAP_LDAP_BASE_DN: "${LLDAP_LDAP_BASE_DN}"
      LLDAP_LDAP_USER_PASS: "${LLDAP_LDAP_USER_PASS}"
      LLDAP_DATABASE_URL: "postgres://lldap:${LLDAP_DB_PASSWORD}@lldap-db/lldap"
      LLDAP_HTTP_URL: "https://lldap.${root_domain}"
      LLDAP_HTTP_PORT: "17170"
      LLDAP_SMTP_OPTIONS__ENABLE_PASSWORD_RESET: "true"
      LLDAP_SMTP_OPTIONS__SERVER: "smtp.sendgrid.net"
      LLDAP_SMTP_OPTIONS__PORT: "465"
      LLDAP_SMTP_OPTIONS__SMTP_ENCRYPTION: TLS
      LLDAP_SMTP_OPTIONS__USER: "apikey"
      LLDAP_SMTP_OPTIONS__PASSWORD: "${SENDGRID_PASS}"
      LLDAP_SMTP_OPTIONS__FROM: "lldap <lldap@${root_domain}>"
      LLDAP_SMTP_OPTIONS__TO: "${letsencrypt_email}"
    labels:
      <<: *traefik
      traefik.docker.network: "${COMPOSE_PROJECT_NAME}_lldap"
      traefik.http.routers.lldap.rule: "Host(`lldap.${root_domain}`)"
      # service
      traefik.http.services.lldap.loadbalancer.server.port: "17170"

  lldap-db:
    <<: *common
    image: docker.io/postgres:16-bookworm
    cpu_count: 2
    mem_limit: 250M
    memswap_limit: 250M
    shm_size: 128mb
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    environment:
      POSTGRES_USER: lldap
      POSTGRES_PASSWORD: ${LLDAP_DB_PASSWORD}
    tmpfs:
      - /run:mode=770,size=5M,uid=${UID},gid=${GID}
      - /tmp:mode=770,size=5M,uid=${UID},gid=${GID}
    volumes:
      - ${docker_data_folder}/lldap-db:/var/lib/postgresql/data
    networks:
      - lldap-db
    healthcheck: *postgres-healthcheck

  # SSO
  authelia:
    <<: *common
    depends_on:
      authelia-db:
        condition: service_healthy
      authelia-valkey:
        condition: service_healthy
      lldap:
        condition: service_healthy
    image: docker.io/authelia/authelia:4.39.1@sha256:e325963609cc928861ffe8130c09111862df88dd8fcafbcd2c47e5ff0a4ae268
    cpu_count: 4
    user: ${UID}:${GID}
    mem_limit: 250M
    memswap_limit: 250M
    cap_drop:
      - ALL
    environment:
      # Environment variables for configuration templating
      ROOT_DOMAIN: "${root_domain}"
      X_AUTHELIA_CONFIG_FILTERS: "template"
      # Global configuration
      AUTHELIA_IDENTITY_VALIDATION_RESET_PASSWORD_JWT_SECRET: ${AUTHELIA_JWT_SECRET}
      AUTHELIA_DEFAULT_REDIRECTION_URL: https://${root_domain}
      # Notifier configuration
      AUTHELIA_NOTIFIER_SMTP_PASSWORD: ${SENDGRID_PASS}
      AUTHELIA_NOTIFIER_SMTP_SENDER: Authelia <authelia@${root_domain}>
      AUTHELIA_NOTIFIER_SMTP_STARTUP_CHECK_ADDRESS: ${letsencrypt_email}
      # LDAP backend configuration
      AUTHELIA_AUTHENTICATION_BACKEND_LDAP_BASE_DN: ${LLDAP_LDAP_BASE_DN}
      AUTHELIA_AUTHENTICATION_BACKEND_LDAP_USER: "uid=authelia,ou=people,${LLDAP_LDAP_BASE_DN}"
      AUTHELIA_AUTHENTICATION_BACKEND_LDAP_PASSWORD: ${AUTHELIA_BIND_PASSWORD}
      AUTHELIA_AUTHENTICATION_BACKEND_LDAP_USERS_FILTER: "(&({username_attribute}={input})(objectclass=person)(memberOf=cn=family,ou=groups,${LLDAP_LDAP_BASE_DN}))"
      # Session configuration
      AUTHELIA_SESSION_DOMAIN: ${root_domain}
      # Storage configuration
      AUTHELIA_STORAGE_ENCRYPTION_KEY: ${AUTHELIA_DB_ENCRYPTION_KEY}
      AUTHELIA_STORAGE_POSTGRES_PASSWORD: ${AUTHELIA_DB_PASSWORD}
      AUTHELIA_SESSION_SECRET: ${AUTHELIA_SESSION_SECRET}
    command:
      - --config
      - /config/config.yml
    volumes:
      - ./config/authelia:/config:ro
    networks:
      - authelia-proxy
      - lldap
      - authelia
      - prometheus-authelia
    labels:
      <<: *traefik
      traefik.docker.network: "${COMPOSE_PROJECT_NAME}_authelia-proxy"
      traefik.http.routers.authelia.rule: Host(`authelia.${root_domain}`)
      # service
      traefik.http.services.authelia.loadbalancer.server.port: "9091"
      traefik.http.middlewares.authelia.forwardauth.address: http://authelia:9091/api/verify?rd=https://authelia.${root_domain}/'
      traefik.http.middlewares.authelia.forwardauth.trustForwardHeader: "true"
      traefik.http.middlewares.authelia.forwardauth.authResponseHeaders: Remote-User, Remote-Groups, Remote-Name, Remote-Email
    healthcheck:
      start_period: 2m

  # DB for authelia
  authelia-db:
    <<: *common
    image: docker.io/postgres:14-bookworm
    cpu_count: 2
    mem_limit: 500M
    memswap_limit: 500M
    shm_size: 128mb
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    environment:
      POSTGRES_USER: authelia
      POSTGRES_PASSWORD: ${AUTHELIA_DB_PASSWORD}
    tmpfs:
      - /run:mode=770,size=5M,uid=${UID},gid=${GID}
      - /tmp:mode=770,size=5M,uid=${UID},gid=${GID}
    volumes:
      - ${docker_data_folder}/authelia-db:/var/lib/postgresql/data
    networks:
      - authelia
    healthcheck: *postgres-healthcheck

  authelia-valkey:
    <<: *common
    cpu_count: 2
    mem_limit: 250M
    memswap_limit: 250M
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    image: docker.io/valkey/valkey:8-alpine
    healthcheck: *valkey-healthcheck
    volumes:
      - ${docker_data_folder}/authelia-redis:/data
    networks:
      - authelia

networks:
  watchtower:
    enable_ipv6: true
  traefik:
    enable_ipv6: true
  authelia:
    enable_ipv6: true
    internal: true
  nextcloud:
    enable_ipv6: true
    internal: true
  homer:
    enable_ipv6: true
    internal: true
  nextcloud-proxy:
    enable_ipv6: true
  robots:
    enable_ipv6: true
    internal: true
  scrutiny:
    enable_ipv6: true
  authelia-proxy:
    enable_ipv6: true
  flaresolverr:
    enable_ipv6: true
  traefik-socket-proxy:
    enable_ipv6: true
    internal: true
  watchtower-socket-proxy:
    enable_ipv6: true
    internal: true
  librespeed:
    enable_ipv6: true
  tandoor:
    enable_ipv6: true
  miniflux:
    enable_ipv6: true
    internal: true
  miniflux-proxy:
    enable_ipv6: true
  lldap:
    enable_ipv6: true
  lldap-db:
    enable_ipv6: true
    internal: true
  prometheus:
    enable_ipv6: true
  grafana:
    enable_ipv6: true
    internal: true
  prometheus-flaresolverr:
    enable_ipv6: true
    internal: true
  prometheus-jellyfin:
    enable_ipv6: true
    internal: true
  prometheus-authelia:
    enable_ipv6: true
    internal: true
  prometheus-miniflux:
    enable_ipv6: true
    internal: true
  prometheus-watchtower:
    enable_ipv6: true
    internal: true

volumes:
  robots-nginx-cache:
  robots-nginx-pid:
  robots-nginx-conf:

secrets:
  miniflux-metrics-password:
    environment: "MINIFLUX_METRICS_PASSWORD"
  watchtower-metrics-password:
    environment: "WATCHTOWER_METRICS_PASSWORD"
