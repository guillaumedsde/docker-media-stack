%YAML 1.1
---
# THE MEDIA STACK #
# this docker-compose file will get a selection of media downloading/management webapps
# I've tried to simplify getting started as much as possible
#   1. configure your desired variables in the .env file
#   2. run `docker compose up -d` from the folder with this docker-copose.yml folder
# NOTE this process is not fully automated and individual configuration of webapps is still required after docker compose

# basic common configuration for most containers
x-conf: &conf
  TZ: ${TZ}
  PGID: ${GID}
  PUID: ${UID}
  # NOTE: this umask env var is mostly used by lsio containers
  #       docker does not have an option to set umask, see:
  #       https://github.com/moby/moby/issues/19189
  UMASK: "077"

# container restart policy
x-common: &common
  restart: always
  logging:
    driver: "local"
    options:
      max-file: "5"
      max-size: "10m"

# Traefik
x-traefik: &traefik
  traefik.enable: "true"

services:
  # reverse proxy
  traefik:
    image: docker.io/traefik:v3.3.3@sha256:f1fdee7fda041872cff24e36a08f45ca53f006ded88f743a8e30e3d87ca52b48
    user: ${UID}:${GID}
    depends_on:
      traefik-socket-proxy:
        condition: service_healthy
      authelia:
        condition: service_healthy
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
    command:
      - "--global.checknewversion=false"
      - "--global.sendanonymoususage=false"
      - "--log.level=${TRAEFIK_LOG_LEVEL:-INFO}"
      # docker configuration
      - "--providers.docker=true"
      - "--providers.docker.endpoint=tcp://traefik-socket-proxy:2375"
      - "--providers.docker.exposedbydefault=false"
      # entrypoints
      # HTTP
      - "--entrypoints.web.address=:80"
      - "--entrypoints.web.http.redirections.entryPoint.to=websecure"
      # HTTPS
      - "--entrypoints.websecure.address=:443"
      - "--entrypoints.websecure.http.tls.certResolver=le"
      - "--entrypoints.websecure.http.tls.domains[0].main=${root_domain}"
      - "--entrypoints.websecure.http.tls.domains[0].sans=*.${root_domain}"
      - "--entryPoints.websecure.http.middlewares=securityHeaders@file"
      # NOTE: setting this to 10min to enable 1GB file upload on 15mb/s connection
      #       see https://github.com/nextcloud/server/issues/37695#issuecomment-2266716070
      - "--entryPoints.websecure.transport.respondingTimeouts.readTimeout=60000s"
      # DNS challenge
      - "--certificatesresolvers.le=true"
      - "--certificatesresolvers.le.acme.storage=/traefik/acme.json"
      - "--certificatesresolvers.le.acme.dnschallenge=true"
      - "--certificatesresolvers.le.acme.dnschallenge.propagation.delayBeforeChecks=30s"
      - "--certificatesresolvers.le.acme.dnschallenge.provider=ovh"
      - "--certificatesresolvers.le.acme.email=${letsencrypt_email}"
      - "--certificatesresolvers.le.acme.keytype=EC384"
      - "--certificatesresolvers.le.acme.dnsChallenge.resolvers=1.1.1.1:53,8.8.8.8:53"
      # configs
      - "--providers.file.directory=/config/"
      # traefik API and dashboard
      - "--api=true"
      - "--api.dashboard=true"
      # traefik healthcheck
      - "--ping=true"
      - "--ping.manualrouting=true"
      # traefik access logs
      # - "--accesslog=true"
    environment:
      OVH_ENDPOINT: ${OVH_ENDPOINT}
      OVH_APPLICATION_KEY: ${OVH_APPLICATION_KEY}
      OVH_APPLICATION_SECRET: ${OVH_APPLICATION_SECRET}
      OVH_CONSUMER_KEY: ${OVH_CONSUMER_KEY}
      LEGO_EXPERIMENTAL_CNAME_SUPPORT: "true"
    labels:
      <<: *traefik
      # Dashboard
      traefik.http.routers.dashboard.rule: Host(`traefik.${root_domain}`)
      traefik.http.routers.dashboard.service: api@internal
      traefik.http.routers.dashboard.middlewares: authelia@docker
      # Healthcheck
      traefik.http.routers.ping.rule: Host(`ping.${root_domain}`)
      traefik.http.routers.ping.service: ping@internal
    read_only: true
    volumes:
      - ${docker_data_folder}/traefik/data:/traefik
      - ./config/traefik:/config:ro
      - ${docker_data_folder}/traefik/plugins-storage:/plugins-storage
    tmpfs:
      - /tmp:mode=770,size=5M,uid=${UID},gid=${GID}
    networks:
      - traefik-socket-proxy
      - traefik
      - homer
      - nextcloud-proxy
      - robots
      - netdata
      - scrutiny
      - authelia-proxy
      - librespeed
      - tandoor
      - miniflux-proxy
      - lldap
    <<: *common
    mem_limit: 500M
    memswap_limit: 500M
    cpu_count: 4
    ports:
      - target: 80
        published: 80
        host_ip: "0.0.0.0"
        protocol: tcp
        mode: host
      - target: 443
        published: 443
        host_ip: "0.0.0.0"
        protocol: tcp
        mode: host
      - target: 80
        published: 80
        host_ip: "::"
        protocol: tcp
        mode: host
      - target: 443
        published: 443
        host_ip: "::"
        protocol: tcp
        mode: host
    healthcheck:
      test:
        - "CMD"
        - "nc"
        - "-vz"
        - "127.0.0.1"
        - "80"
      start_period: 3s
      interval: 3s
      timeout: 3s
      retries: 5

  traefik-socket-proxy: &socket-proxy
    <<: *common
    image: docker.io/wollomatic/socket-proxy:1.5.4@sha256:cd8b18ed2ca7ad9a3e45b87fadaa3eec01bed845a31d693b2827085b80a20aec
    cpu_count: 2
    mem_limit: 50M
    memswap_limit: 50M
    user: ${UID}:${docker_group_id}
    read_only: true
    cap_drop:
      - ALL
    security_opt:
      - no-new-privileges
    volumes:
      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock
        read_only: true
    entrypoint:
      - "/socket-proxy"
      - "-loglevel=info"
      - "-listenip=0.0.0.0"
      - "-shutdowngracetime=5"
      - "-watchdoginterval=600"
      - "-stoponwatchdog"
      - "-allowhealthcheck"
    command:
      - "-allowfrom=traefik"
      # NOTE: https://github.com/wollomatic/traefik-hardened/blob/b2bfcad2a1bf727f2fea33d3d68d3eca9bb1f8da/docker-compose.yaml#L12
      - '-allowGET=/v1\..{1,2}/(version|containers/.*|events.*)'
    networks:
      - traefik-socket-proxy
    healthcheck:
      test:
        - "CMD"
        - "./healthcheck"
      interval: 10s
      timeout: 5s
      retries: 2

  # watchtower container to update images
  # NOTE to update the containers images' watchtower needs to restart the containers this implies some downtime when it does
  # NOTE watchtower also updates itself
  watchtower:
    image: ghcr.io/containrrr/watchtower:1.7.1@sha256:f9086bfda061100361fc2bacf069585678d760d705cf390918ccdbda8a00980b
    <<: *common
    mem_limit: 50M
    memswap_limit: 50M
    cpu_count: 2
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    environment:
      DOCKER_HOST: "tcp://watchtower-socket-proxy:2375"
    depends_on:
      watchtower-socket-proxy:
        condition: service_healthy
    networks:
      - watchtower
      - watchtower-socket-proxy
    command:
      - "--cleanup"
      - "--schedule"
      - "0 0 5 * * *"

  watchtower-socket-proxy:
    <<: *socket-proxy
    command:
      - "-allowfrom=watchtower"
      # NOTE: https://github.com/wollomatic/socket-proxy/blob/6ba2fa6112fb9a115e99de0634b15f1fbe1fb513/examples/docker-compose/watchtower/compose.yaml#L9-L12
      - '-allowGET=/v1\..{2}/(containers/.*|images/.*)'
      - '-allowPOST=/v1\..{2}/(containers/.*|images/.*|networks/.*)'
      - '-allowDELETE=/v1\..{2}/(containers/.*|images/.*)'
    networks:
      - watchtower-socket-proxy

  wireguard-tun-device-creator:
    image: docker.io/busybox:1-musl
    cpu_count: 2
    mem_limit: 20M
    memswap_limit: 20M
    read_only: true
    volumes:
      - type: bind
        source: ${docker_data_folder}/gluetun
        target: /gluetun
    entrypoint:
      - "/bin/sh"
      - "-c"
    command:
      - |
        rm -f "/gluetun/tun"
        mknod "/gluetun/tun" c 10 200

  # Wireguard VPN client
  wireguard:
    <<: *common
    depends_on:
      wireguard-tun-device-creator:
        condition: service_completed_successfully
    image: docker.io/qmcgaw/gluetun:v3.40.0@sha256:2b42bfa046757145a5155acece417b65b4443c8033fb88661a8e9dcf7fda5a00
    cap_drop:
      - ALL
    cap_add:
      - NET_ADMIN
    cpu_count: 4
    mem_limit: 250M
    memswap_limit: 250M
    sysctls:
      net.ipv6.conf.all.disable_ipv6: 0
    volumes:
      - type: bind
        source: ${docker_data_folder}/gluetun
        target: /gluetun
    devices:
      - "${docker_data_folder}/gluetun/tun:/dev/net/tun"
    networks:
      traefik:
        aliases:
          - qbittorrent
          - prowlarr
      flaresolverr:
    healthcheck:
      retries: 25
      start_period: 30s
    environment:
      <<: *conf
      LOG_LEVEL: ${GLUETUN_LOG_LEVEL:-info}
      VPN_SERVICE_PROVIDER: ${VPN_SERVICE_PROVIDER}
      VPN_TYPE: wireguard
      SERVER_COUNTRIES: ${VPN_SERVER_COUNTRIES}
      WIREGUARD_PRIVATE_KEY: ${WIREGUARD_PRIVATE_KEY}
      WIREGUARD_PRESHARED_KEY: ${WIREGUARD_PRESHARED_KEY}
      WIREGUARD_ADDRESSES: ${WIREGUARD_ADDRESSES}
      FIREWALL_VPN_INPUT_PORTS: ${FIREWALL_VPN_INPUT_PORTS}
      # Allow LAN access to other containers
      FIREWALL_OUTBOUND_SUBNETS: ${FIREWALL_OUTBOUND_SUBNETS}
      DNS_KEEP_NAMESERVER: "on"
      VERSION_INFORMATION: "off"
      HEALTH_SUCCESS_WAIT_DURATION: "10m"
    labels:
      <<: *traefik
      traefik.docker.network: "${COMPOSE_PROJECT_NAME}_traefik"
      # Prowlarr
      # service
      traefik.http.services.prowlarr.loadbalancer.server.port: "9696"
      # Router
      traefik.http.routers.prowlarr.rule: Host(`prowlarr.${root_domain}`)
      traefik.http.routers.prowlarr.service: prowlarr
      # Forward auth
      traefik.http.routers.prowlarr.middlewares: authelia@docker
      # qBittorrent
      # service
      traefik.http.services.qbittorrent.loadbalancer.server.port: "8080"
      # router
      traefik.http.routers.qbittorrent.rule: Host(`torrent.${root_domain}`)
      traefik.http.routers.qbittorrent.service: qbittorrent
      # Forward auth
      traefik.http.routers.qbittorrent.middlewares: authelia@docker
      # Disable watchtower for this container because of these issues
      # https://github.com/containrrr/watchtower/issues/188
      # https://github.com/containrrr/watchtower/issues/1013
      com.centurylinklabs.watchtower.enable: "false"

  # Torrent client
  qbittorrent:
    <<: *common
    depends_on:
      wireguard:
        condition: service_healthy
    volumes:
      - ${docker_data_folder}/qbittorrent:/config
      - ${data_folder}:/data
      - ${docker_data_folder}/blackhole:/blackhole
      - /etc/localtime:/etc/localtime:ro
    tmpfs:
      - /tmp:mode=770,size=5M,uid=${UID},gid=${GID}
    read_only: true
    cpu_count: 6
    mem_limit: 10G
    memswap_limit: 10G
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    # NOTE: use `services:` instead of `containers:` because the
    #       container is defined in the same compose file
    network_mode: service:wireguard
    image: ghcr.io/guillaumedsde/qbittorrent-distroless@sha256:3825dffeedd1bf2918fba7fa918048159c3fb3a00cb980e4954ea9cd92bb440c
    labels:
      com.centurylinklabs.watchtower.depends-on: ${COMPOSE_PROJECT_NAME}-wireguard-1
    healthcheck:
      start_period: 30s
      retries: 10

  prowlarr:
    <<: *common
    image: ghcr.io/elfhosted/prowlarr-develop:1.30.2.4939@sha256:96c66163cb96614f2f7a49e34c8317dd489cb6168aa6150febc2d1ac407da97a
    cpu_count: 4
    mem_limit: 3G
    memswap_limit: 3G
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    network_mode: service:wireguard
    depends_on:
      wireguard:
        condition: service_healthy
    volumes:
      - ${docker_data_folder}/prowlarr:/config
      - ${docker_data_folder}/blackhole:/blackhole
    environment:
      <<: *conf
    labels:
      com.centurylinklabs.watchtower.depends-on: ${COMPOSE_PROJECT_NAME}-wireguard-1
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:9696/ping
      interval: 10s
      timeout: 5s
      retries: 8
      start_period: 15s

  flaresolverr:
    image: ghcr.io/flaresolverr/flaresolverr:v3.3.21@sha256:1a30e1ad6bb3df626bfe3b9735a6d60e208c475c89bf18be0db4d4b121a3cb0e
    <<: *common
    # NOTE: default image UID is 1000
    user: 1000:1000
    cpu_count: 4
    mem_limit: 2G
    memswap_limit: 2G
    cap_drop:
      - ALL
    environment:
      <<: *conf
    networks:
      - flaresolverr

  # radarr, webapp for downloading movies through torrent client
  radarr:
    image: ghcr.io/elfhosted/radarr:5.18.4.9674@sha256:95c59bf66bf048ffcc12804b109a20b033dd06529f0c2c351dd68400dc75d4a1
    <<: *common
    cpu_count: 4
    mem_limit: 2G
    memswap_limit: 2G
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    depends_on:
      qbittorrent:
        condition: service_healthy
      prowlarr:
        condition: service_healthy
    volumes:
      - ${docker_data_folder}/radarr:/config
      - ${data_folder}:/data
    networks:
      - traefik
    environment:
      <<: *conf
    labels:
      <<: *traefik
      traefik.http.routers.radarr.rule: Host(`movies.${root_domain}`)
      # service
      traefik.http.services.radarr.loadbalancer.server.port: "7878"
      # Forward auth
      traefik.http.routers.radarr.middlewares: authelia@docker
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:7878/ping
      interval: 5s
      timeout: 4s
      retries: 4
      start_period: 5s

  # sonarr, webapp for downloading series through torrent client
  sonarr:
    <<: *common
    image: ghcr.io/elfhosted/sonarr-develop:4.0.12.2900@sha256:8af82e2eb00add0f0a902c57852fc33ab9aabb15595545e93d3c9f4ebea2e189
    cpu_count: 4
    mem_limit: 2G
    memswap_limit: 2G
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    depends_on:
      qbittorrent:
        condition: service_healthy
      prowlarr:
        condition: service_healthy
    volumes:
      - ${docker_data_folder}/sonarr:/config
      - ${data_folder}:/data
    networks:
      - traefik
    environment:
      <<: *conf
      # NOTE: fix for read-only filesystems, see:
      #       https://stackoverflow.com/questions/74447989/failed-to-create-coreclr-hresult-0x80004005#comment131446515_74447989
      COMPlus_EnableDiagnostics: "0"
    labels:
      <<: *traefik
      traefik.http.routers.sonarr.rule: Host(`series.${root_domain}`)
      # service
      traefik.http.services.sonarr.loadbalancer.server.port: "8989"
      # Forward auth
      traefik.http.routers.sonarr.middlewares: authelia@docker
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:8989/ping
      interval: 5s
      timeout: 4s
      retries: 4
      start_period: 5s

  # lidarr, webapp for downloading music through torrent client
  lidarr:
    <<: *common
    image: ghcr.io/elfhosted/lidarr:2.9.6.4552@sha256:77103d41d3e1572f87492af71b514535b0ba9e47c24534170c1a9895a7e9f37e
    cpu_count: 4
    mem_limit: 2G
    memswap_limit: 2G
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    depends_on:
      qbittorrent:
        condition: service_healthy
      prowlarr:
        condition: service_healthy
    volumes:
      - ${docker_data_folder}/lidarr:/config
      - ${data_folder}:/data
    networks:
      - traefik
    environment:
      <<: *conf
      # NOTE: fix for read-only filesystems, see:
      #       https://stackoverflow.com/questions/74447989/failed-to-create-coreclr-hresult-0x80004005#comment131446515_74447989
      COMPlus_EnableDiagnostics: "0"
    labels:
      <<: *traefik
      traefik.http.routers.lidarr.rule: Host(`music.${root_domain}`)
      # service
      traefik.http.services.lidarr.loadbalancer.server.port: "8686"
      # Forward auth
      traefik.http.routers.lidarr.middlewares: authelia@docker
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:8686/ping
      interval: 5s
      timeout: 4s
      retries: 4
      start_period: 5s

  # readarr, webapp for downloading books through torrent client
  readarr:
    <<: *common
    image: ghcr.io/elfhosted/readarr-develop:0.4.10.2734@sha256:980f84ad232f288483837508bbc4a90a18875e9095d3c621755d80103953b195
    cpu_count: 4
    mem_limit: 2G
    memswap_limit: 2G
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    depends_on:
      qbittorrent:
        condition: service_healthy
      prowlarr:
        condition: service_healthy
    volumes:
      - ${docker_data_folder}/readarr:/config
      - ${data_folder}:/data
    networks:
      - traefik
    environment:
      <<: *conf
      # NOTE: fix for read-only filesystems, see:
      #       https://stackoverflow.com/questions/74447989/failed-to-create-coreclr-hresult-0x80004005#comment131446515_74447989
      COMPlus_EnableDiagnostics: "0"
    labels:
      <<: *traefik
      traefik.http.routers.readarr.rule: Host(`books.${root_domain}`)
      # service
      traefik.http.services.readarr.loadbalancer.server.port: "8787"
      # Forward auth
      traefik.http.routers.readarr.middlewares: authelia@docker
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:8787/ping
      interval: 5s
      timeout: 4s
      retries: 4
      start_period: 5s

  # bazarr, webapp for downloading subtitles
  bazarr:
    <<: *common
    depends_on:
      radarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    image: ghcr.io/elfhosted/bazarr:1.5.1@sha256:b3019fd3d8f62ebb61540f9be1625e3602de8e8c1c7b87d531f65e8379eafb0f
    cpu_count: 2
    mem_limit: 500M
    memswap_limit: 500M
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    environment:
      <<: *conf
    volumes:
      - ${docker_data_folder}/bazarr:/config
      - ${data_folder}:/data
    networks:
      - traefik
    labels:
      <<: *traefik
      traefik.http.routers.bazarr.rule: Host(`subtitles.${root_domain}`)
      # service
      traefik.http.services.bazarr.loadbalancer.server.port: "6767"
      # Forward auth
      traefik.http.routers.bazarr.middlewares: authelia@docker
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://127.0.0.1:6767
      interval: 5s
      timeout: 4s
      retries: 4
      start_period: 5s

  # automatically extract torrented archives
  unpackerr:
    <<: *common
    image: docker.io/golift/unpackerr:0.14.5@sha256:8493ffc2dd17e0b8a034552bb52d44e003fa457ee407da97ccc69328bce4a815
    user: ${UID}:${GID}
    read_only: true
    cpu_count: 4
    mem_limit: 500M
    memswap_limit: 500M
    cap_drop:
      - ALL
    networks:
      - traefik
    volumes:
      - ${data_folder}:/data
    environment:
      UN_SONARR_0_URL: http://sonarr:8989
      UN_SONARR_0_API_KEY: ${SONARR_API_KEY}
      UN_SONARR_0_PATH: /data/download/series
      UN_RADARR_0_URL: http://radarr:7878
      UN_RADARR_0_API_KEY: ${RADARR_API_KEY}
      UN_RADARR_0_PATH: /data/download/series
      UN_LIDARR_0_URL: http://lidarr:8686
      UN_LIDARR_0_API_KEY: ${LIDARR_API_KEY}
      UN_LIDARR_0_PATH: /data/download/music

  # homer is a homepage container
  homer:
    <<: *common
    image: docker.io/b4bz/homer:v25.02.1@sha256:cf4ae752adbbcbcc93a2bfc6c651b7b9c998ded38ba115cbab1d161dc3cfcb73
    user: ${UID}:${GID}
    cpu_count: 2
    mem_limit: 100M
    memswap_limit: 100M
    read_only: true
    cap_drop:
      - ALL
    networks:
      - homer
    volumes:
      - ${docker_data_folder}/homer/config.yml:/www/assets/config.yml:ro
      - ${docker_data_folder}/homer/assets:/www/assets:ro
    labels:
      <<: *traefik
      traefik.http.routers.homer.rule: Host(`home.${root_domain}`) || Host(`${root_domain}`)
      # service
      traefik.http.services.homer.loadbalancer.server.port: "8080"
      # Forward auth
      traefik.http.routers.homer.middlewares: authelia@docker

  # jellyfin media server
  jellyfin:
    <<: *common
    image: docker.io/jellyfin/jellyfin:10.10.5@sha256:89d020f73334642943eca3269d2a652ad897481bdf0bd9827e108c939af49f24
    user: ${UID}:${GID}
    cpu_count: 15
    mem_limit: 8G
    memswap_limit: 8G
    read_only: true
    cap_drop:
      - ALL
    depends_on:
      lldap:
        condition: service_healthy
    tmpfs:
      - /config/data/transcodes:mode=770,size=4G,uid=${UID},gid=${GID}
      - /tmp:mode=770,size=10M,uid=${UID},gid=${GID}
    volumes:
      - type: bind
        source: ${docker_data_folder}/jellyfin
        target: /config
      - type: bind
        source: ${data_folder}/movies
        target: /data/movies
        read_only: true
      - type: bind
        source: ${data_folder}/series
        target: /data/series
        read_only: true
      - type: bind
        source: ${data_folder}/music
        target: /data/music
        read_only: true
      - type: bind
        source: ${data_folder}/pictures
        target: /data/pictures
        read_only: true
      - type: bind
        source: "${data_folder}/Recettes de cuisine"
        target: /data/recipes
        read_only: true
    networks:
      - traefik
      - lldap
    environment:
      <<: *conf
      # NOTE: required for read only root filesystem, see:
      #       https://github.com/dotnet/docs/issues/10217#issuecomment-462323277
      COMPlus_EnableDiagnostics: "0"
      #Uncomment forofficial Jellyfin image:
      JELLYFIN_DATA_DIR: /config/data
      JELLYFIN_CONFIG_DIR: /config
      JELLYFIN_LOG_DIR: /config/log
      JELLYFIN_CACHE_DIR: /config/cache
      JELLYFIN_FFmpeg__probesize: "2G"
      JELLYFIN_FFmpeg__analyzeduration: "800M"
      JELLYFIN_PublishedServerUrl: https://jellyfin.${root_domain}
    labels:
      <<: *traefik
      traefik.docker.network: "${COMPOSE_PROJECT_NAME}_traefik"
      traefik.http.routers.jellyfin.rule: Host(`jellyfin.${root_domain}`)
      # service
      traefik.http.services.jellyfin.loadbalancer.server.port: "8096"

  jellyseer:
    <<: *common
    image: docker.io/fallenbagel/jellyseerr:2.3.0@sha256:df53a7b06006e9da117a7072a55cf5d8b2071a6272f6bb329d8ca62b6f5c08a6
    user: ${UID}:${GID}
    cpu_count: 2
    mem_limit: 500M
    memswap_limit: 500M
    read_only: true
    cap_drop:
      - ALL
    depends_on:
      jellyfin:
        condition: service_healthy
      radarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    volumes:
      - ${docker_data_folder}/jellyseer:/app/config
    tmpfs:
      - /tmp:mode=770,size=50M,uid=${UID},gid=${GID}
    networks:
      - traefik
    environment:
      <<: *conf
    labels:
      <<: *traefik
      traefik.http.middlewares.ombi-jellyseer-redirect.redirectregex.regex: "^https?://ombi.${root_domain}/(.*)"
      traefik.http.middlewares.ombi-jellyseer-redirect.redirectregex.replacement: "https://jellyseer.${root_domain}/"
      traefik.http.middlewares.ombi-jellyseer-redirect.redirectregex.permanent: "false"
      traefik.http.routers.jellyseer.rule: Host(`jellyseer.${root_domain}`) || Host(`ombi.${root_domain}`)
      # service
      traefik.http.services.jellyseer.loadbalancer.server.port: "5055"
      # Forward auth
      traefik.http.routers.jellyseer.middlewares: ombi-jellyseer-redirect
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:5055
      interval: 5s
      timeout: 4s
      retries: 4
      start_period: 5s

  # personal cloud
  nextcloud: &nextcloud
    <<: *common
    depends_on:
      nextcloud-postgres:
        condition: service_healthy
      nextcloud-redis:
        condition: service_healthy
      lldap:
        condition: service_healthy
    mem_limit: 4G
    memswap_limit: 4G
    cpu_count: 8
    image: docker.io/nextcloud:30.0-apache
    user: ${NEXTCLOUD_UID}:${GID}
    cap_drop:
      - ALL
    read_only: true
    tmpfs:
      - /run:mode=770,size=5M,uid=${NEXTCLOUD_UID},gid=${GID}
      # NOTE: /tmp needs to be proportional to max uploadable file size, see:
      #       https://docs.nextcloud.com/server/latest/admin_manual/configuration_files/big_file_upload_configuration.html#system-configuration
      - /tmp:mode=770,size=4000M,uid=${NEXTCLOUD_UID},gid=${GID}
    volumes:
      - type: "bind"
        source: "${docker_data_folder}/nextcloud"
        target: "/var/www/html"
      - type: "bind"
        source: "${user_data_folder}"
        target: "/data/users"
      - type: "bind"
        source: "${docker_data_folder}/nextcloud/redis-session.ini"
        target: "/usr/local/etc/php/conf.d/redis-session.ini"
    networks:
      - traefik
      - nextcloud
      - nextcloud-proxy
      - lldap
    environment:
      <<: *conf
      POSTGRES_DB: nextcloud
      POSTGRES_USER: nextcloud
      POSTGRES_PASSWORD: ${NEXTCLOUD_DB_PASSWORD}
      POSTGRES_HOST: nextcloud-postgres
      REDIS_HOST: nextcloud-redis
      REDIS_HOST_PASSWORD: ${NEXTCLOUD_REDIS_PASS}
      SMTP_HOST: smtp.sendgrid.net
      SMTP_SECURE: ssl
      SMTP_PORT: "465"
      SMTP_AUTHTYPE: login
      SMTP_NAME: apikey
      SMTP_PASSWORD: ${SENDGRID_PASS}
      MAIL_FROM_ADDRESS: nextcloud
      MAIL_DOMAIN: ${root_domain}
      PHP_UPLOAD_LIMIT: 1G
      APACHE_BODY_LIMIT: "1073741824"
    labels:
      <<: *traefik
      traefik.docker.network: "${COMPOSE_PROJECT_NAME}_nextcloud-proxy"
      traefik.http.routers.nextcloud.rule: Host(`cloud.${root_domain}`)
      traefik.http.routers.nextcloud.middlewares: nextcloud-redirectregex
      # service
      traefik.http.services.nextcloud.loadbalancer.server.port: "80"
      traefik.http.middlewares.nextcloud-redirectregex.redirectregex.permanent: "true"
      traefik.http.middlewares.nextcloud-redirectregex.redirectregex.regex: https://cloud.${root_domain}/.well-known/(card|cal)dav
      traefik.http.middlewares.nextcloud-redirectregex.redirectregex.replacement: https://cloud.${root_domain}/remote.php/dav/
    healthcheck:
      test: curl -sSf 'http://localhost/status.php' | grep '"installed":true' | grep '"maintenance":false' | grep '"needsDbUpgrade":false' || exit 1
      interval: 10s
      timeout: 5s
      retries: 10

  nextcloud-cron:
    <<: *nextcloud
    labels: {}
    entrypoint:
      - /cron.sh
    healthcheck: {}

  # database for nextcloud
  nextcloud-postgres:
    <<: *common
    image: docker.io/postgres:15-bookworm
    cpu_count: 2
    mem_limit: 2G
    memswap_limit: 2G
    shm_size: 128mb
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    environment:
      POSTGRES_PASSWORD: ${NEXTCLOUD_DB_PASSWORD}
      POSTGRES_USER: nextcloud
    tmpfs:
      - /run:mode=770,size=5M,uid=${UID},gid=${GID}
      - /tmp:mode=770,size=5M,uid=${UID},gid=${GID}
    volumes:
      - ${docker_data_folder}/nextcloud-postgres:/var/lib/postgresql/data
    networks:
      - nextcloud
    healthcheck: &postgres-healthcheck
      test:
        - "CMD-SHELL"
        - >-
          pg_isready
          --username="$${POSTGRES_USER}"
          --dbname="$${POSTGRES_USER}"
          --host=localhost
          --port=5432
      start_period: 5s
      interval: 5s
      timeout: 4s
      retries: 5

  # cache for nextcloud
  nextcloud-redis:
    <<: *common
    cpu_count: 2
    mem_limit: 250M
    memswap_limit: 250M
    read_only: true
    user: ${UID}:${GID}
    image: docker.io/valkey/valkey:8-alpine
    cap_drop:
      - ALL
    networks:
      - nextcloud
    volumes:
      - ${docker_data_folder}/nextcloud-redis:/data
    environment:
      REDISCLI_AUTH: "${NEXTCLOUD_REDIS_PASS}"
      VALKEY_EXTRA_FLAGS: "--requirepass ${NEXTCLOUD_REDIS_PASS}"
    healthcheck: &valkey-healthcheck
      test:
        - "CMD"
        - "valkey-cli"
        - "ping"

  miniflux:
    <<: *common
    depends_on:
      miniflux-db:
        condition: service_healthy
    image: ghcr.io/miniflux/miniflux:2.2.5-distroless@sha256:ae1ec65014ee5d0a9a6c0910868aca617917a085cb9a9e5d61040b417379b4ba
    cpu_count: 2
    mem_limit: 500M
    memswap_limit: 500M
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    networks:
      - miniflux
      - miniflux-proxy
    environment:
      BASE_URL: "https://miniflux.${root_domain}/"
      AUTH_PROXY_HEADER: "Remote-User"
      AUTH_PROXY_USER_CREATION: "true"
      RUN_MIGRATIONS: "1"
      DATABASE_URL: "postgres://miniflux:${MINIFLUX_DB_PASSWORD}@miniflux-db:5432/miniflux?sslmode=disable"
    labels:
      <<: *traefik
      traefik.docker.network: "${COMPOSE_PROJECT_NAME}_miniflux-proxy"
      traefik.http.routers.miniflux.rule: Host(`miniflux.${root_domain}`)
      # service
      traefik.http.services.miniflux.loadbalancer.server.port: "8080"
      # Forward auth
      traefik.http.routers.miniflux.middlewares: authelia@docker

  miniflux-db:
    <<: *common
    image: docker.io/postgres:16-bookworm
    cpu_count: 2
    mem_limit: 2G
    memswap_limit: 2G
    shm_size: 128mb
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    environment:
      POSTGRES_PASSWORD: ${MINIFLUX_DB_PASSWORD}
      POSTGRES_USER: miniflux
    tmpfs:
      - /run:mode=770,size=5M,uid=${UID},gid=${GID}
      - /tmp:mode=770,size=5M,uid=${UID},gid=${GID}
    volumes:
      - ${docker_data_folder}/miniflux-db:/var/lib/postgresql/data
    networks:
      - miniflux
    healthcheck: *postgres-healthcheck

  librespeed:
    <<: *common
    image: ghcr.io/librespeed/speedtest-rust:v1.3.7@sha256:c4d18c0fe1892efcbfc7cb917cfa0a5ffd5ba8b616243d980c5755cb4f08edce
    cpu_count: 2
    mem_limit: 500M
    memswap_limit: 500M
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    volumes:
      - type: bind
        source: "./config/librespeed-rs/config.toml"
        target: "/usr/local/bin/configs.toml"
        read_only: true
    networks:
      - librespeed
    labels:
      <<: *traefik
      traefik.http.routers.librespeed.rule: Host(`librespeed.${root_domain}`)
      # service
      traefik.http.services.librespeed.loadbalancer.server.port: "8080"
      # Forward auth
      traefik.http.routers.librespeed.middlewares: authelia@docker

  tandoor-db:
    <<: *common
    image: docker.io/postgres:16-bookworm
    cpu_count: 2
    mem_limit: 500M
    memswap_limit: 500M
    shm_size: 128mb
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    environment:
      POSTGRES_USER: tandoor
      POSTGRES_PASSWORD: ${TANDOOR_DB_PASSWORD}
    tmpfs:
      - /run:mode=770,size=5M,uid=${UID},gid=${GID}
      - /tmp:mode=770,size=5M,uid=${UID},gid=${GID}
    volumes:
      - ${docker_data_folder}/tandoor-db:/var/lib/postgresql/data
    networks:
      - tandoor
    healthcheck: *postgres-healthcheck

  tandoor:
    <<: *common
    image: docker.io/vabene1111/recipes:1.5.31@sha256:063eb446e2981a737c8722642acef6aeacbd7b33f672a982209bd82c068b1a2c
    cpu_count: 2
    mem_limit: 500M
    memswap_limit: 500M
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    depends_on:
      tandoor-db:
        condition: service_healthy
    environment:
      SECRET_KEY: ${TANDOOR_SECRET_KEY}
      DB_ENGINE: django.db.backends.postgresql
      POSTGRES_HOST: tandoor-db
      POSTGRES_PORT: "5432"
      POSTGRES_USER: tandoor
      POSTGRES_PASSWORD: ${TANDOOR_DB_PASSWORD}
      POSTGRES_DB: tandoor
      REMOTE_USER_AUTH: "1"
    volumes:
      - type: bind
        source: "${docker_data_folder}/tandoor/staticfiles"
        target: "/opt/recipes/staticfiles"
      - type: bind
        source: "${docker_data_folder}/tandoor/mediafiles"
        target: "/opt/recipes/mediafiles"
    tmpfs:
      - /tmp:mode=770,size=10M,uid=${UID},gid=${GID}
    networks:
      - tandoor
    labels:
      <<: *traefik
      traefik.http.routers.tandoor.rule: Host(`tandoor.${root_domain}`)
      # service
      traefik.http.services.tandoor.loadbalancer.server.port: "8080"
      # Forward auth
      traefik.http.routers.tandoor.middlewares: authelia@docker
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --timeout=3
        - --tries=1
        - --spider
        - http://localhost:8080
      interval: 5s
      timeout: 4s
      retries: 4
      start_period: 5s

  robots:
    <<: *common
    image: docker.io/nginxinc/nginx-unprivileged:1.27-alpine-slim
    mem_limit: 200M
    memswap_limit: 200M
    cpu_count: 2
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    tmpfs:
      - /var/cache/nginx:mode=770,size=5M,uid=${UID},gid=${GID}
      - /var/run:mode=770,size=5M,uid=${UID},gid=${GID}
      - /tmp:mode=770,size=5M,uid=${UID},gid=${GID}
    volumes:
      - type: bind
        source: ./config/nginx/default.conf
        target: /etc/nginx/conf.d/default.conf
        read_only: true
      - type: bind
        source: ./config/nginx/robots.txt
        target: /usr/share/nginx/html/robots.txt
        read_only: true
    networks:
      - robots
    labels:
      <<: *traefik
      traefik.http.routers.robots.rule: ( Host(`${root_domain}`) || HostRegexp(`{subhost:[a-z]+}.${root_domain}`) ) && Path(`/robots.txt`)
      # service
      traefik.http.services.robots.loadbalancer.server.port: "8080"
    healthcheck:
      test:
        - "CMD"
        - "nc"
        - "-vz"
        - "127.0.0.1"
        - "8080"
      start_period: 3s
      interval: 1s
      timeout: 1s
      retries: 5

  ##################################################################
  # Monitoring

  # netdata, server monitoring
  netdata:
    <<: *common
    hostname: "status.gauf.fr"
    cpu_count: 2
    mem_limit: 1G
    memswap_limit: 1G
    pid: host
    image: docker.io/netdata/netdata:v2.2.5@sha256:bb76ddafea0e1a80686ea1021fd1554317132bc1b1b5336975e2716b5620e5ce
    volumes:
      - /:/host/root:ro,rslave
      - /etc/passwd:/host/etc/passwd:ro
      - /etc/group:/host/etc/group:ro
      - /etc/localtime:/etc/localtime:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /etc/os-release:/host/etc/os-release:ro
      - /var/log:/host/var/log:ro
      - /run/dbus:/run/dbus:ro
      # NOTE: see https://learn.netdata.cloud/docs/netdata-agent/installation/docker#change-the-default-hostname
      - /etc/hostname:/host/etc/hostname:ro
      - ${docker_data_folder}/netdata/health_alarm_notify.conf:/etc/netdata/health_alarm_notify.conf
      - ${docker_data_folder}/netdata/cache:/var/cache/netdata
      - ${docker_data_folder}/netdata/netdatalib:/var/lib/netdata
    depends_on:
      netdata-socket-proxy:
        condition: service_healthy
    networks:
      - netdata
      - netdata-socket-proxy
    environment:
      <<: *conf
      DOCKER_HOST: netdata-socket-proxy:2375
      NETDATA_DISABLE_TELEMETRY: "1"
      DISABLE_TELEMETRY: "1"
      DO_NOT_TRACK: "1"
      NETDATA_DISABLE_CLOUD: "1"
    labels:
      <<: *traefik
      traefik.docker.network: "${COMPOSE_PROJECT_NAME}_netdata"
      traefik.http.routers.netdata.rule: Host(`status.${root_domain}`)
      # service
      traefik.http.services.netdata.loadbalancer.server.port: "19999"
      # CSP HTTP header middleware
      traefik.http.middlewares.netdataCSP.headers.customresponseheaders.Content-Security-Policy:
        >-
        Content-Security-Policy:
        base-uri 'self';
        default-src 'self' 'unsafe-inline' 'unsafe-eval';
      # Forward auth
      traefik.http.routers.netdata.middlewares: authelia@docker,netdataCSP@docker
    cap_add:
      - SYS_PTRACE
      - SYS_ADMIN
    security_opt:
      - apparmor:unconfined

  netdata-socket-proxy:
    <<: *socket-proxy
    command:
      - "-allowfrom=netdata"
      - '-allowGET=/v1\..{2}/(containers/.*|images/.*|info)'
      # NOTE: for some reason netdata accesses non-versionned docker endpoints
      - "-allowGET=/containers/.*"
    networks:
      - netdata-socket-proxy

  scrutiny:
    image: ghcr.io/analogj/scrutiny:v0.8.1-omnibus@sha256:214261df881879941b4b3e2235c601b086b358d0a29b513a82c4bf1e2de68e3e
    <<: *common
    mem_limit: 500M
    memswap_limit: 500M
    cpu_count: 2
    cap_add:
      - SYS_RAWIO
    devices:
      - /dev/sda
      - /dev/sdb
      - /dev/sdc
      - /dev/sdd
      - /dev/sde
      - /dev/sdf
      - /dev/sdg
      - /dev/sdh
      - /dev/sdi
      - /dev/sdj
      - /dev/sdk
      - /dev/sdl
    volumes:
      - /run/udev:/run/udev:ro
      - ${docker_data_folder}/scrutiny/scrutiny:/opt/scrutiny/config
      - ${docker_data_folder}/scrutiny/influxdb:/opt/scrutiny/influxdb
    networks:
      - scrutiny
    environment:
      <<: *conf
    labels:
      <<: *traefik
      traefik.http.routers.scrutiny.rule: Host(`scrutiny.${root_domain}`)
      # service
      traefik.http.services.scrutiny.loadbalancer.server.port: "8080"
      # Forward auth
      traefik.http.routers.scrutiny.middlewares: authelia@docker
    healthcheck:
      test:
        - CMD
        - curl
        - -sSf
        - http://localhost:8080
      interval: 5s
      timeout: 4s
      retries: 4
      start_period: 5s

  ##################################################################
  # Authentication

  lldap:
    <<: *common
    depends_on:
      lldap-db:
        condition: service_healthy
    image: "docker.io/lldap/lldap:v0.6.1-alpine-rootless@sha256:7e1c16090167c49d34e9e98f9f7d3fce97d3d86de8d2f62eedefad3dca3d270b"
    cpu_count: 2
    mem_limit: "250M"
    memswap_limit: "250M"
    user: "${UID}:${GID}"
    read_only: true
    cap_drop:
      - ALL
    volumes:
      - type: "bind"
        source: "${docker_data_folder}/lldap"
        target: /data
    networks:
      - "lldap"
      - "lldap-db"
    environment:
      <<: *conf
      LLDAP_LDAP_HOST: "::"
      LLDAP_LDAP_PORT: "3890"
      LLDAP_JWT_SECRET: "${LLDAP_JWT_SECRET}"
      LLDAP_KEY_SEED: "${LLDAP_KEY_SEED}"
      LLDAP_LDAP_BASE_DN: "${LLDAP_LDAP_BASE_DN}"
      LLDAP_LDAP_USER_PASS: "${LLDAP_LDAP_USER_PASS}"
      LLDAP_DATABASE_URL: "postgres://lldap:${LLDAP_DB_PASSWORD}@lldap-db/lldap"
      LLDAP_HTTP_URL: "https://lldap.${root_domain}"
      LLDAP_HTTP_PORT: "17170"
      LLDAP_SMTP_OPTIONS__ENABLE_PASSWORD_RESET: "true"
      LLDAP_SMTP_OPTIONS__SERVER: "smtp.sendgrid.net"
      LLDAP_SMTP_OPTIONS__PORT: "465"
      LLDAP_SMTP_OPTIONS__SMTP_ENCRYPTION: TLS
      LLDAP_SMTP_OPTIONS__USER: "apikey"
      LLDAP_SMTP_OPTIONS__PASSWORD: "${SENDGRID_PASS}"
      LLDAP_SMTP_OPTIONS__FROM: "lldap <lldap@${root_domain}>"
      LLDAP_SMTP_OPTIONS__TO: "${letsencrypt_email}"
    labels:
      <<: *traefik
      traefik.docker.network: "${COMPOSE_PROJECT_NAME}_lldap"
      traefik.http.routers.lldap.rule: "Host(`lldap.${root_domain}`)"
      # service
      traefik.http.services.lldap.loadbalancer.server.port: "17170"

  lldap-db:
    <<: *common
    image: docker.io/postgres:16-bookworm
    cpu_count: 2
    mem_limit: 250M
    memswap_limit: 250M
    shm_size: 128mb
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    environment:
      POSTGRES_USER: lldap
      POSTGRES_PASSWORD: ${LLDAP_DB_PASSWORD}
    tmpfs:
      - /run:mode=770,size=5M,uid=${UID},gid=${GID}
      - /tmp:mode=770,size=5M,uid=${UID},gid=${GID}
    volumes:
      - ${docker_data_folder}/lldap-db:/var/lib/postgresql/data
    networks:
      - lldap-db
    healthcheck: *postgres-healthcheck

  # SSO
  authelia:
    <<: *common
    depends_on:
      authelia-db:
        condition: service_healthy
      authelia-valkey:
        condition: service_healthy
      lldap:
        condition: service_healthy
    image: docker.io/authelia/authelia:4.38.18@sha256:a5e6fa4b1dbdc296f80f9175157b145a0598690ec8cd4d6e105cd4498fe0c731
    cpu_count: 4
    user: ${UID}:${GID}
    mem_limit: 250M
    memswap_limit: 250M
    cap_drop:
      - ALL
    environment:
      # Global configuration
      AUTHELIA_IDENTITY_VALIDATION_RESET_PASSWORD_JWT_SECRET: ${AUTHELIA_JWT_SECRET}
      AUTHELIA_DEFAULT_REDIRECTION_URL: https://${root_domain}
      # Notifier configuration
      AUTHELIA_NOTIFIER_SMTP_PASSWORD: ${SENDGRID_PASS}
      AUTHELIA_NOTIFIER_SMTP_SENDER: Authelia <authelia@${root_domain}>
      AUTHELIA_NOTIFIER_SMTP_STARTUP_CHECK_ADDRESS: ${letsencrypt_email}
      # LDAP backend configuration
      AUTHELIA_AUTHENTICATION_BACKEND_LDAP_BASE_DN: ${LLDAP_LDAP_BASE_DN}
      AUTHELIA_AUTHENTICATION_BACKEND_LDAP_USER: "uid=authelia,ou=people,${LLDAP_LDAP_BASE_DN}"
      AUTHELIA_AUTHENTICATION_BACKEND_LDAP_PASSWORD: ${AUTHELIA_BIND_PASSWORD}
      # Session configuration
      AUTHELIA_SESSION_DOMAIN: ${root_domain}
      # Storage configuration
      AUTHELIA_STORAGE_ENCRYPTION_KEY: ${AUTHELIA_DB_ENCRYPTION_KEY}
      AUTHELIA_STORAGE_POSTGRES_PASSWORD: ${AUTHELIA_DB_PASSWORD}
      AUTHELIA_SESSION_SECRET: ${AUTHELIA_SESSION_SECRET}
    command:
      - --config
      - /config/config.yml
    volumes:
      - ./config/authelia:/config:ro
    networks:
      - authelia-proxy
      - lldap
      - authelia
    labels:
      <<: *traefik
      traefik.docker.network: "${COMPOSE_PROJECT_NAME}_authelia-proxy"
      traefik.http.routers.authelia.rule: Host(`authelia.${root_domain}`)
      # service
      traefik.http.services.authelia.loadbalancer.server.port: "9091"
      traefik.http.middlewares.authelia.forwardauth.address: http://authelia:9091/api/verify?rd=https://authelia.${root_domain}/'
      traefik.http.middlewares.authelia.forwardauth.trustForwardHeader: "true"
      traefik.http.middlewares.authelia.forwardauth.authResponseHeaders: Remote-User, Remote-Groups, Remote-Name, Remote-Email
    healthcheck:
      start_period: 2m

  # DB for authelia
  authelia-db:
    <<: *common
    image: docker.io/postgres:14-bookworm
    cpu_count: 2
    mem_limit: 500M
    memswap_limit: 500M
    shm_size: 128mb
    user: ${UID}:${GID}
    read_only: true
    cap_drop:
      - ALL
    environment:
      POSTGRES_USER: authelia
      POSTGRES_PASSWORD: ${AUTHELIA_DB_PASSWORD}
    tmpfs:
      - /run:mode=770,size=5M,uid=${UID},gid=${GID}
      - /tmp:mode=770,size=5M,uid=${UID},gid=${GID}
    volumes:
      - ${docker_data_folder}/authelia-db:/var/lib/postgresql/data
    networks:
      - authelia
    healthcheck: *postgres-healthcheck

  authelia-valkey:
    <<: *common
    cpu_count: 2
    mem_limit: 250M
    memswap_limit: 250M
    read_only: true
    user: ${UID}:${GID}
    cap_drop:
      - ALL
    image: docker.io/valkey/valkey:8-alpine
    healthcheck: *valkey-healthcheck
    volumes:
      - ${docker_data_folder}/authelia-redis:/data
    networks:
      - authelia

networks:
  watchtower:
    enable_ipv6: true
  traefik:
    enable_ipv6: true
  authelia:
    enable_ipv6: true
    internal: true
  nextcloud:
    enable_ipv6: true
    internal: true
  homer:
    enable_ipv6: true
    internal: true
  nextcloud-proxy:
    enable_ipv6: true
  robots:
    enable_ipv6: true
    internal: true
  netdata:
    enable_ipv6: true
    internal: true
  scrutiny:
    enable_ipv6: true
  authelia-proxy:
    enable_ipv6: true
  flaresolverr:
    enable_ipv6: true
  traefik-socket-proxy:
    enable_ipv6: true
    internal: true
  watchtower-socket-proxy:
    enable_ipv6: true
    internal: true
  netdata-socket-proxy:
    enable_ipv6: true
    internal: true
  librespeed:
    enable_ipv6: true
  tandoor:
    enable_ipv6: true
  miniflux:
    enable_ipv6: true
    internal: true
  miniflux-proxy:
    enable_ipv6: true
  lldap:
    enable_ipv6: true
  lldap-db:
    enable_ipv6: true
    internal: true

volumes:
  robots-nginx-cache:
  robots-nginx-pid:
  robots-nginx-conf:
